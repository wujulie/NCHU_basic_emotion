{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Trainer.ipynb","provenance":[],"authorship_tag":"ABX9TyNJlv4DseCVljCS3vfcLu7M"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"code","metadata":{"id":"srZszaL2B_Zi"},"source":["from keras import backend as K\r\n","from keras.layers.normalization import BatchNormalization\r\n","from keras.layers import Flatten, concatenate, Input, Dropout, Dense, Activation, MaxPooling2D, Conv2D, \\\r\n","    AveragePooling2D, GlobalAveragePooling2D,  LSTM, TimeDistributed, merge, LeakyReLU\r\n","from keras import optimizers\r\n","from keras.regularizers import l2\r\n","from keras.models import Model, load_model\r\n","from keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\r\n","from keras.utils import np_utils, plot_model, get_custom_objects\r\n","from keras.preprocessing.image import ImageDataGenerator, load_img, img_to_array\r\n","from sklearn.utils import shuffle, class_weight\r\n","from sklearn.model_selection import StratifiedKFold, train_test_split, KFold\r\n","from sklearn.metrics import classification_report, confusion_matrix\r\n","import gc\r\n","import itertools\r\n","import numpy as np\r\n","import matplotlib.pyplot as plt\r\n","import pandas as pd\r\n","from sklearn.preprocessing import StandardScaler\r\n","from Package.Data import Data, DataType\r\n","\r\n","## 訓練器\r\n","class Trainer(object):\r\n","    def __init__(self, img_width=224, img_height=224, num_classes=7, epochs=50, batch_size=96, data=Data, model=None):\r\n","        self.img_height = img_height\r\n","        self.img_width = img_width\r\n","        self.num_classes = num_classes\r\n","        self.epochs = epochs\r\n","        self.batch_size = batch_size\r\n","        self.set_model(model)\r\n","        self.data = data\r\n","\r\n","    # 設定模型\r\n","    def set_model(self, model):\r\n","        self._model = model\r\n","\r\n","    # 取得模型\r\n","    def get_model(self):\r\n","        return self._model\r\n","\r\n","        # 訓練模型\r\n","\r\n","    def fit(self, argument_amount=5, per_epoch_amount=220, filename=\"model\", use_steps=False):\r\n","        if (use_steps):\r\n","            history = self._model.fit_generator(self.data.generator(batch_size=self.batch_size),\r\n","                                                epochs=self.epochs,\r\n","                                                #  steps_per_epoch=int(np.ceil(train_x.shape[0] / float(self.batch_size))),\r\n","                                                workers=4,\r\n","                                                # shuffle=False,\r\n","                                                samples_per_epoch=(len(self.data.train_x) * argument_amount),\r\n","                                                initial_epoch=0,\r\n","                                                validation_data=(self.data.val_x, self.data.val_y),\r\n","                                                # validation_steps=self.batch_size,\r\n","                                                callbacks=self.callbacks,\r\n","                                                verbose=1,\r\n","                                                # class_weight=self.class_weight\r\n","                                                )\r\n","        else:\r\n","            history = self._model.fit_generator(self.data.generator(batch_size=self.batch_size),\r\n","                                                epochs=self.epochs,\r\n","                                                steps_per_epoch=per_epoch_amount,\r\n","                                                #  steps_per_epoch=int(np.ceil(train_x.shape[0] / float(self.batch_size))),\r\n","                                                workers=4,\r\n","                                                # shuffle=False,\r\n","                                                initial_epoch=0,\r\n","                                                validation_data=(self.data.val_x, self.data.val_y),\r\n","                                                # validation_steps=self.batch_size,\r\n","                                                callbacks=self.callbacks,\r\n","                                                verbose=1,\r\n","                                                # class_weight=self.class_weight\r\n","                                                )\r\n","        pd.DataFrame(history.history).to_csv('{}_history.csv'.format(filename))\r\n","        # 建立檢查點與Early Stopping\r\n","\r\n","    def set_callbacks(self, filename,\r\n","                      check_point=ModelCheckpoint(\"callbacks.{epoch:02d}-{val_acc:.5f}.hdf5\", monitor='val_acc',\r\n","                                                  verbose=1,\r\n","                                                  save_best_only=True, mode='max'),\r\n","                      earlyStopping=EarlyStopping(monitor='val_acc', min_delta=0.001, patience=10, verbose=2,\r\n","                                                  mode='max'),\r\n","                      reduce_lr=ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=2, min_lr=1E-9)):\r\n","        origin_list = [check_point, earlyStopping, reduce_lr]\r\n","        callbacks = []\r\n","        for item in origin_list:\r\n","            if item is not None:\r\n","                callbacks.append(item)\r\n","        self.callbacks = callbacks"],"execution_count":null,"outputs":[]}]}