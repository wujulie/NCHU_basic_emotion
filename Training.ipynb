{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Training.ipynb","provenance":[],"mount_file_id":"1Zv7XRz8w4e4Eftf67eskQIxIa8VrttD_","authorship_tag":"ABX9TyMAceVjUFT9w0mb4XzVGrfz"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"ZfS31Vx5knER"},"source":["Data.py 資料"]},{"cell_type":"code","metadata":{"id":"k4bbikXRjz_6","executionInfo":{"status":"ok","timestamp":1611799516375,"user_tz":-480,"elapsed":4140,"user":{"displayName":"吳貞宜","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhIOzgYhbporG1Eax-zD0HNdC2UZr8CRTfHsZcY5g=s64","userId":"00889427696288622203"}}},"source":["import itertools\r\n","\r\n","import cv2\r\n","import dlib\r\n","import numpy as np\r\n","import matplotlib.pyplot as plt\r\n","import pandas as pd\r\n","import time\r\n","from keras.preprocessing.image import ImageDataGenerator\r\n","from keras.utils import np_utils\r\n","from sklearn.model_selection import train_test_split\r\n","from sklearn.metrics import confusion_matrix\r\n","from sklearn.preprocessing import StandardScaler\r\n","from enum import Enum\r\n","# 輸入資料型態\r\n","from sklearn.utils import shuffle\r\n","import os\r\n","# 資料類別\r\n","class DataType(Enum):\r\n","    ALL_DATA = 'all data'       # 所有資料\r\n","    TRAIN = 'train data'        # 只有訓練資料\r\n","    TEST = 'test data'          # 只有測試資料\r\n","    CUSTOM = 'custom data'      # 原先設計好之資料\r\n","    SMALL = 'small dataset'     # 小型資料集\r\n","\r\n","# 輸入資料\r\n","class Data(object):\r\n","    def __init__(self, data_type=DataType.ALL_DATA,\r\n","                 img_width=224, img_height=224,\r\n","                 split_data=False, filename='small_data', num_classes=7,#訓練時須修改標籤數目\r\n","                 data_generator=True):\r\n","        print(\"- create data instance\")\r\n","        try:\r\n","            print(\"\\t- set data type ... \", end=\"\")\r\n","            self.data_type=data_type\r\n","            self.num_classes=num_classes\r\n","            self.filename = filename\r\n","            print(\"{}\".format(data_type.name))\r\n","        except:\r\n","            print(\"\")\r\n","        try:\r\n","            print('\\t- set X and Y ... ', end=\"\")\r\n","            self.x = np.load('/content/drive/MyDrive/basic_emotion/' + filename + '_x.npy')\r\n","            self.y = np.load('/content/drive/MyDrive/basic_emotion/' + filename + '_y.npy')\r\n","            print(\"OK\")\r\n","        except  Exception as e:\r\n","            print(e)\r\n","        try:\r\n","            print(\"\\t- set image shape ... \", end=\"\")\r\n","            self.img_width=img_width\r\n","            self.img_height=img_height\r\n","            print(\"OK\")\r\n","        except Exception as e:\r\n","            print(\"image shape setting error\")\r\n","            print(e)\r\n","        self.split_data=split_data\r\n","        try:\r\n","            print(\"\\t- start data pre process ... \", end=\"\")\r\n","            self.data_generator = data_generator\r\n","            self.pre_process()\r\n","            print(\"OK\")\r\n","        except  Exception as e:\r\n","            print(\"error\")\r\n","            print(e)\r\n","\r\n","\r\n","    # 資料預處理\r\n","    def pre_process(self, test_size=0.2, val_size=0.1, filename=None):\r\n","        x, y = shuffle(self.x, self.y)\r\n","        x = x.reshape(x.shape[0], self.img_width, self.img_height, 1)\r\n","        if (self.data_type == DataType.ALL_DATA):\r\n","            train_x, test_x, train_y, test_y = train_test_split(x, y, test_size=test_size, random_state=25)\r\n","            train_x, val_x, train_y, val_y = train_test_split(x, y, test_size=val_size, random_state=25)\r\n","        if (self.data_type == DataType.TRAIN):\r\n","            train_x, test_x, train_y, test_y = train_test_split(x, y, test_size=test_size, random_state=25)\r\n","            val_x = test_x\r\n","            val_y = test_y\r\n","        if (self.data_type == DataType.TEST or self.data_type == DataType.SMALL):\r\n","            train_x = x\r\n","            train_y = y\r\n","            test_x = x\r\n","            test_y = y\r\n","            val_x = x\r\n","            val_y = y\r\n","\r\n","        if (self.data_type == DataType.CUSTOM):\r\n","            self.train_x = self.arr_reshape(x)\r\n","            self.train_y = y\r\n","            self.test_x, self.test_y = shuffle(np.load('/content/drive/MyDrive/basic_emotion/{}_test_x.npy'.format(filename)), np.load('/content/drive/MyDrive/basic_emotion/{}_test_y.npy'.format(filename)))\r\n","            self.val_x, self.val_y = shuffle(np.load('/content/drive/MyDrive/basic_emotion/{}_val_x.npy'.format(filename)), np.load('/content/drive/MyDrive/basic_emotion/{}_val_y.npy'.format(filename)))\r\n","            self.test_x = self.arr_reshape(self.test_x)\r\n","            self.val_x = self.arr_reshape(self.val_x)\r\n","\r\n","        self.train_x = train_x\r\n","        self.train_y = train_y\r\n","        self.val_x = val_x\r\n","        self.val_y = val_y\r\n","        self.test_x = test_x\r\n","        self.test_y = test_y\r\n","        self.one_hot_encoding()\r\n","        self.save_data()\r\n","\r\n","    # 對輸出值進行 one hot encodcing\r\n","    def one_hot_encoding(self):\r\n","        try:\r\n","            pass\r\n","            self.train_y = np_utils.to_categorical(self.train_y, self.num_classes).astype('int')\r\n","            self.test_y = np_utils.to_categorical(self.test_y, self.num_classes).astype('int')\r\n","            self.val_y = np_utils.to_categorical(self.val_y, self.num_classes).astype('int')\r\n","        except  Exception as e:\r\n","            print(e)\r\n","\r\n","    # 對 numpy 進行矩陣重組\r\n","    def arr_reshape(self, arr):\r\n","        return arr.reshape(arr.shape[0], self.img_width, self.img_height, 1)\r\n","\r\n","    # 靜態取用矩陣重組\r\n","    @classmethod\r\n","    def static_arr_reshape(self, arr, img_width=224, img_height=224):\r\n","        return arr.reshape(arr.shape[0], img_width, img_height, 1)\r\n","\r\n","    # 靜態取用 one hot encoding\r\n","    @classmethod\r\n","    def static_one_hot_encoding(self, y=None, classes=6):\r\n","        return np_utils.to_categorical(y, classes).astype('int')\r\n","\r\n","    # 資料擴增\r\n","    def generator(self, batch_size):\r\n","        # train_datagen = ImageDataGenerator(horizontal_flip=True, rotation_range=10, zoom_range=0.2)\r\n","        # train_datagen = ImageDataGenerator(horizontal_flip=True, rotation_range=10)\r\n","        if self.data_generator:\r\n","            train_datagen = ImageDataGenerator(horizontal_flip=True, rotation_range=10, zoom_range=0.2)\r\n","            # train_datagen = ImageDataGenerator(horizontal_flip=True, rotation_range=10)\r\n","            train_datagen.fit(self.train_x)\r\n","            train_generator = train_datagen.flow(self.train_x, self.train_y, batch_size=batch_size)\r\n","            return train_generator\r\n","        else:\r\n","            train_datagen = ImageDataGenerator()\r\n","            train_datagen.fit(self.train_x)\r\n","            train_generator = train_datagen.flow(self.train_x, self.train_y, batch_size=batch_size)\r\n","            return train_generator\r\n","\r\n","    # 資料儲存\r\n","    def save_data(self):\r\n","        np.save('/content/drive/MyDrive/basic_emotion/' + self.filename + '_train_x.npy', self.train_x)#10/13改過\r\n","        np.save('/content/drive/MyDrive/basic_emotion/' + self.filename + '_train_y.npy', self.train_y)\r\n","        np.save('/content/drive/MyDrive/basic_emotion/' + self.filename + '_val_x.npy', self.val_x)\r\n","        np.save('/content/drive/MyDrive/basic_emotion/' + self.filename + '_val_y.npy', self.val_y)\r\n","        np.save('/content/drive/MyDrive/basic_emotion/' + self.filename + '_test_x.npy', self.test_x)\r\n","        np.save('/content/drive/MyDrive/basic_emotion/' + self.filename + '_test_y.npy', self.test_y)\r\n","\r\n","    # 將圖片轉換成矩陣\r\n","    @classmethod\r\n","    def load_data(self, path='', filename=''):\r\n","        folders, x, y= [], [], []\r\n","        for root, dirs, files in os.walk(path, topdown=False):\r\n","            for name in dirs:\r\n","                folders.append(name)\r\n","        folders = folders[-7:]\r\n","        start_time = time.time()\r\n","        count = 0\r\n","        for fld in folders:\r\n","            index = folders.index(fld)\r\n","            for root, dirs, files in os.walk(path + '\\\\' + fld, topdown=False):\r\n","                for file in files:\r\n","                    filepath = root + '\\\\' + file\r\n","                    img = cv2.imread(filepath)\r\n","                    img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\r\n","                    resized = cv2.resize(img, (224, 224), cv2.INTER_LANCZOS4)\r\n","                    y.append(index)\r\n","                    x.append(resized)\r\n","        np.save('{}_X.npy'.format(filename), x)\r\n","        np.save('{}_Y.npy'.format(filename), y)\r\n","\r\n","\r\n","\r\n","    # 預測圖片轉換矩陣\r\n","    @classmethod\r\n","    def load_train_data(self, path=\"D:\\\\NianXiang_File\\\\CNN\\\\Dataset\\\\small_dataset\\\\NEW\", filename=\"\"):\r\n","        x = []\r\n","        y = []\r\n","        folders = []\r\n","        for root, dirs, files in os.walk(path, topdown=False):\r\n","            for name in dirs:\r\n","                folders.append(name)\r\n","        folders = folders[-7:]\r\n","        start_time = time.time()\r\n","        count = 0\r\n","        for fld in folders:\r\n","            index = folders.index(fld)\r\n","            print('Loading {} files (Index: {})'.format(fld, index))\r\n","            for root, dirs, files in os.walk(path + '\\\\' + fld, topdown=False):\r\n","                for file in files:\r\n","                    print(file)\r\n","                    if (file == '1.jpg'):\r\n","                        pass\r\n","                    else:\r\n","                        filepath = root + '\\\\' + file\r\n","                        img = cv2.imread(filepath)\r\n","                        try:\r\n","                            img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\r\n","                            resized = cv2.resize(img, (224, 224), cv2.INTER_LANCZOS4)\r\n","                            y.append(index)\r\n","                            x.append(resized)\r\n","                            count = count + 1\r\n","                        except:\r\n","                            pass\r\n","\r\n","                            # file = Data.static_load_image(ffile)\r\n","        np.save('{}_x.npy'.format(filename), x)\r\n","        np.save('{}_y.npy'.format(filename), y)\r\n","        print(\"Complete\")\r\n","    # 預測圖片轉換矩陣\r\n","    @classmethod\r\n","    def load_predict_data(self, path=\"\", filename=\"\"):\r\n","        x = []\r\n","        for root, dirs, files in os.walk(path, topdown=False):\r\n","            for file in files:\r\n","                filepath = root + '\\\\' + file\r\n","                img = cv2.imread(filepath)\r\n","                img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\r\n","                resized = cv2.resize(img, (224, 224), cv2.INTER_LANCZOS4)\r\n","                x.append(resized)\r\n","        np.save('{}_predict_X.npy'.format(filename), x)\r\n","    # 將圖片抓取臉部後轉換成矩陣\r\n","    @classmethod\r\n","    def get_data_from_path(self, path='D:\\\\NianXiang_File\\\\CNN\\\\Dataset\\\\LE', file_name='BE'):\r\n","        # data = np.load('predict_y_nor.npy')\r\n","        output = {}\r\n","        x = []\r\n","        y = []\r\n","        folders = []\r\n","        for root, dirs, files in os.walk(path, topdown=False):\r\n","            for name in dirs:\r\n","                folders.append(name)\r\n","        folders = folders[-7:]\r\n","        start_time = time.time()\r\n","        count = 0\r\n","        for fld in folders:\r\n","            index = folders.index(fld)\r\n","            print('Loading {} files (Index: {})'.format(fld, index))\r\n","            for root, dirs, files in os.walk(path + '\\\\' + fld, topdown=False):\r\n","                for file in files:\r\n","                    print(file)\r\n","                    if (file == '1.jpg'):\r\n","                        pass\r\n","                    else:\r\n","                        filepath = root + '\\\\' + file\r\n","                        detector = dlib.get_frontal_face_detector()\r\n","                        img = cv2.imread(filepath)\r\n","                        face = detector(img, 1)\r\n","                        for i, d in enumerate(face):\r\n","                            x1 = d.left()\r\n","                            y1 = d.top()\r\n","                            x2 = d.right()\r\n","                            y2 = d.bottom()\r\n","                            img = img[y1:y2, x1:x2]\r\n","                        lpath = 'D:\\\\NianXiang_File\\\\CNN\\\\Dataset\\\\small_dataset\\\\NEW'\r\n","                        filename = lpath + '\\\\' + str(index) + '\\\\' + str(count) + '.jpg'\r\n","                        cv2.imwrite(filename, img)\r\n","                        try:\r\n","                            img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\r\n","                            resized = cv2.resize(img, (224, 224), cv2.INTER_LANCZOS4)\r\n","                            y.append(index)\r\n","                            x.append(resized)\r\n","                            count = count + 1\r\n","                        except:\r\n","                            pass\r\n","\r\n","                            # file = Data.static_load_image(ffile)\r\n","        np.save('{}_X.npy'.format(file_name), x)\r\n","        np.save('{}_Y.npy'.format(file_name), y)\r\n","        print(\"Complete\")\r\n","\r\n","class Evaluate(object):\r\n","    # 評估模型準確率\r\n","    @classmethod\r\n","    def evaluate(self, model, x=None, y=None):\r\n","        loss, score = model.evaluate(x=x, y=y, batch_size=16, verbose=2)\r\n","        print(\"Accuracy：{:.2f}%\".format(score * 100))\r\n","    # 預測資料\r\n","    @classmethod\r\n","    def predict(self, model=None, x=None):\r\n","        predict = model.predict(x=x, batch_size=16, verbose=2)\r\n","        return predict\r\n","    # 混淆矩陣\r\n","    @classmethod\r\n","    def confusion_matrix(self, model=None, normalize=False, title='Confusion Matrix', cls='le', x=None, y=None):\r\n","        predict = model.predict(x=x, batch_size=16, verbose=2)\r\n","        pred = np.argmax(predict, axis=1)\r\n","        print(np.unique(np.argmax(y, axis=1)), np.unique(pred))\r\n","        cnf_matrix = confusion_matrix(np.argmax(y, axis=1), pred)\r\n","        np.set_printoptions(precision=2)\r\n","        plt.figure()\r\n","        if cls == 'be':\r\n","            class_name = ['Angry', 'Disgust', 'Fear', 'Happy', 'Sad', 'Surprise', 'Neutral']\r\n","        if cls == 'le':\r\n","            class_name = [\"Frustration\", \"Confused\", \"Bored\", \"Delightful\", \"Flow\", \"Surprise\"]\r\n","        Evaluate.__plot_confusion_matrix(cnf_matrix, classes=class_name,\r\n","                                title=title,\r\n","                                normalize=normalize)\r\n","        plt.show()\r\n","\r\n","    @classmethod\r\n","    def __plot_confusion_matrix(self, cm, classes,\r\n","                                normalize=False,\r\n","                                title='Confusion matrix',\r\n","                                cmap=plt.cm.Blues):\r\n","        \"\"\"\r\n","        This function prints and plots the confusion matrix.\r\n","        Normalization can be applied by setting `normalize=True`.\r\n","        \"\"\"\r\n","        if normalize:\r\n","            cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\r\n","            print(\"Normalized confusion matrix\")\r\n","        else:\r\n","            print('Confusion matrix, without normalization')\r\n","        print(cm)\r\n","\r\n","        plt.imshow(cm, interpolation='nearest', cmap=cmap)\r\n","        plt.title(title)\r\n","        plt.colorbar()\r\n","        tick_marks = np.arange(len(classes))\r\n","        plt.xticks(tick_marks, classes, rotation=45)\r\n","        plt.yticks(tick_marks, classes)\r\n","\r\n","        fmt = '.2f' if normalize else 'd'\r\n","        thresh = cm.max() / 2.\r\n","        for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\r\n","            plt.text(j, i, format(cm[i, j], fmt),\r\n","                     horizontalalignment=\"center\",\r\n","                     color=\"white\" if cm[i, j] > thresh else \"black\")\r\n","\r\n","        plt.tight_layout()\r\n","        plt.ylabel('True label')\r\n","        plt.xlabel('Predicted label')"],"execution_count":1,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"LpnuSRydkaAG"},"source":["**Train.py 訓練器**"]},{"cell_type":"code","metadata":{"id":"rRXx-QBIkcJj","executionInfo":{"status":"ok","timestamp":1611799516376,"user_tz":-480,"elapsed":4135,"user":{"displayName":"吳貞宜","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhIOzgYhbporG1Eax-zD0HNdC2UZr8CRTfHsZcY5g=s64","userId":"00889427696288622203"}}},"source":["from keras import backend as K\r\n","from keras.layers.normalization import BatchNormalization\r\n","from keras.layers import Flatten, concatenate, Input, Dropout, Dense, Activation, MaxPooling2D, Conv2D, \\\r\n","    AveragePooling2D, GlobalAveragePooling2D, LSTM, TimeDistributed, merge, LeakyReLU\r\n","from keras import optimizers\r\n","from keras.regularizers import l2\r\n","from keras.models import Model, load_model\r\n","from keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\r\n","from keras.utils import np_utils, plot_model, get_custom_objects\r\n","from keras.preprocessing.image import ImageDataGenerator, load_img, img_to_array\r\n","from sklearn.utils import shuffle, class_weight\r\n","from sklearn.model_selection import StratifiedKFold, train_test_split, KFold, cross_val_score\r\n","from sklearn.metrics import classification_report, confusion_matrix\r\n","import gc\r\n","import itertools\r\n","import numpy as np\r\n","import matplotlib.pyplot as plt\r\n","import pandas as pd\r\n","from sklearn.preprocessing import StandardScaler\r\n","# from Package.Data import Data, DataType\r\n","from sklearn.neighbors import KNeighborsClassifier\r\n","\r\n","## 訓練器\r\n","class Trainer(object):\r\n","    def __init__(self, img_width=224, img_height=224, num_classes=7, epochs=50, batch_size=96, data=Data, model=None):\r\n","        self.img_height = img_height\r\n","        self.img_width = img_width\r\n","        self.num_classes = num_classes\r\n","        self.epochs = epochs\r\n","        self.batch_size = batch_size\r\n","        self.set_model(model)\r\n","        self.data = data\r\n","    # 設定模型\r\n","    def set_model(self, model):\r\n","        self._model = model\r\n","\r\n","    # 取得模型\r\n","    def get_model(self):\r\n","        return self._model\r\n","\r\n","    # 訓練模型\r\n","    def fit(self, argument_amount=5, per_epoch_amount=220, filename=\"model\", use_steps=False):\r\n","        if (use_steps):\r\n","            history = self._model.fit_generator(self.data.generator(batch_size=self.batch_size),\r\n","                                                epochs=self.epochs,\r\n","                                                #  steps_per_epoch=int(np.ceil(train_x.shape[0] / float(self.batch_size))),\r\n","                                                workers=4,\r\n","                                                # shuffle=False,\r\n","                                                samples_per_epoch=(len(self.data.train_x) * argument_amount),\r\n","                                                initial_epoch=0,\r\n","                                                validation_data=(self.data.val_x, self.data.val_y),\r\n","                                                # validation_steps=self.batch_size,\r\n","                                                callbacks=self.callbacks,\r\n","                                                verbose=1,\r\n","                                                # class_weight=self.class_weight\r\n","                                                )\r\n","        else:\r\n","            history = self._model.fit_generator(self.data.generator(batch_size=self.batch_size),\r\n","                                                epochs=self.epochs,\r\n","                                                steps_per_epoch=per_epoch_amount,\r\n","                                                #  steps_per_epoch=int(np.ceil(train_x.shape[0] / float(self.batch_size))),\r\n","                                                workers=4,\r\n","                                                # shuffle=False,\r\n","                                                initial_epoch=0,\r\n","                                                validation_data=(self.data.val_x, self.data.val_y),\r\n","                                                # validation_steps=self.batch_size,\r\n","                                                callbacks=self.callbacks,\r\n","                                                verbose=1,\r\n","                                                # class_weight=self.class_weight\r\n","                                                )\r\n","        pd.DataFrame(history.history).to_csv('{}_history.csv'.format(filename))\r\n","    # 建立檢查點與Early Stopping\r\n","    def set_callbacks(self, filename,\r\n","                      check_point=ModelCheckpoint(\"callbacks.{epoch:02d}-{val_acc:.5f}.hdf5\", monitor='val_acc',\r\n","                                         verbose=1,\r\n","                                         save_best_only=True, mode='max'),\r\n","                      earlyStopping=EarlyStopping(monitor='val_acc', min_delta=0.001, patience=10, verbose=2, mode='max'),\r\n","                      reduce_lr=ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=2, min_lr=1E-9)):\r\n","        origin_list = [check_point, earlyStopping, reduce_lr]\r\n","        callbacks = []\r\n","        for item in origin_list:\r\n","            if item is not None:\r\n","                callbacks.append(item)\r\n","        self.callbacks = callbacks"],"execution_count":2,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"_Y-mxx0OnUEu"},"source":["Model.py 模型"]},{"cell_type":"code","metadata":{"id":"InjuAqIZnWVi","executionInfo":{"status":"ok","timestamp":1611799516804,"user_tz":-480,"elapsed":4560,"user":{"displayName":"吳貞宜","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhIOzgYhbporG1Eax-zD0HNdC2UZr8CRTfHsZcY5g=s64","userId":"00889427696288622203"}}},"source":["from keras import backend as K\r\n","from keras.layers.normalization import BatchNormalization\r\n","from keras.layers import Flatten, concatenate, Input, Dropout, Dense, Activation, MaxPooling2D, Conv2D, \\\r\n","    AveragePooling2D, GlobalAveragePooling2D, LSTM, TimeDistributed, merge, LeakyReLU\r\n","from keras import optimizers\r\n","from keras.regularizers import l2\r\n","from keras.models import Model, load_model\r\n","from keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\r\n","from keras.utils import np_utils, plot_model, get_custom_objects\r\n","from keras.preprocessing.image import ImageDataGenerator, load_img, img_to_array\r\n","from sklearn.utils import shuffle, class_weight\r\n","from sklearn.model_selection import StratifiedKFold, train_test_split, KFold\r\n","from sklearn.metrics import classification_report, confusion_matrix\r\n","import gc\r\n","import itertools\r\n","import numpy as np\r\n","import matplotlib.pyplot as plt\r\n","import pandas as pd\r\n","from sklearn.preprocessing import StandardScaler\r\n","\r\n","\r\n","def swish(x):\r\n","    return K.sigmoid(x) * x\r\n","\r\n","\r\n","class Swish(Activation):\r\n","    def __init__(self, activation, **kwargs):\r\n","        super(Swish, self).__init__(activation, **kwargs)\r\n","        self.__name__ = 'swish'\r\n","\r\n","\r\n","get_custom_objects().update({\"swish\": Swish(swish)})\r\n","\r\n","\r\n","# 模型類別\r\n","class CustomModel(object):\r\n","    # 卷積層 + Batch Normalization Layer + Activate Function\r\n","    def conv2d_bn(self, x, filters, num_row, num_col, padding='same', strides=(1, 1), name=None,\r\n","                  activate='relu'):\r\n","        if name is not None:\r\n","            bn_name = name + '_bn'\r\n","            conv_name = name + '_conv'\r\n","        else:\r\n","            bn_name = None\r\n","            conv_name = None\r\n","        x = Conv2D(\r\n","            filters, (num_row, num_col),\r\n","            strides=strides,\r\n","            padding=padding,\r\n","            use_bias=False,\r\n","            kernel_initializer='he_normal',  # globor_uniform,\r\n","            kernel_regularizer = l2(0.0001),\r\n","            name = conv_name)(x)\r\n","        x = BatchNormalization(scale=True, name=bn_name)(x)\r\n","        if (activate != None):\r\n","            x = Activation(activate, name=name)(x)\r\n","        return x\r\n","\r\n","    # 激活模型\r\n","    def model_compile(self, loss='categorical_crossentropy', opt='adamax', met=['acc']):\r\n","        self._model.compile(loss=loss, optimizer=opt, metrics=met)\r\n","\r\n","    # 取得模型\r\n","    def get_model(self):\r\n","        return self._model\r\n","\r\n","    # 設定模型\r\n","    def set_model(self, model):\r\n","        self._model = model\r\n","\r\n","    # 遷移學習\r\n","    def transfer_learning(self, model):\r\n","        model.layers.pop()\r\n","        model.layers.pop()\r\n","        x = model.layers[-1].output\r\n","        x = Dense(7, activation='softmax', name='probbb')(x)#訓練時要改，基本為7，學習為6\r\n","        new_model = Model(inputs=model.input, outputs=x)\r\n","        self._model = new_model\r\n","\r\n","# 論文中提出之架構\r\n","class Dense_FaceLiveNet(CustomModel):\r\n","    def __init__(self, activate='swish', use_dense_block=True, use_global_average_pool=True):\r\n","        # 初始化\r\n","        #   Parameter\r\n","        #   1. activate                 : 激活函數\r\n","        #   2. use_dense_block          : 是否使用 Dense Block，若沒有使用，則為原本 FaceLiveNet 所使用之 Residual Block\r\n","        #   3. use_global_average_pool  : 是否使用 GlobalAveragePool，若沒有使用，則使用全連接層\r\n","        self.activate = activate\r\n","        self.use_dense_block = use_dense_block\r\n","        self.use_global_average_pool = use_global_average_pool\r\n","        self._model = self.build()\r\n","        self.model_compile()\r\n","\r\n","    # 建立模型架構\r\n","    def build(self):\r\n","        get_custom_objects().update({\"swish\": Swish(swish)})\r\n","        # Backend 為 Tensorflow 定義 channel axis 為 3\r\n","        channel_axis = 3\r\n","        # 定義 Input 的大小\r\n","        input_shape = Input(shape=(224, 224, 1), name='data')\r\n","\r\n","        # Stem layer\r\n","        net = self.conv2d_bn(input_shape, 32, 3, 3, strides=(2, 2), padding='valid', activate=self.activate)\r\n","        net = self.conv2d_bn(net, 32, 3, 3, strides=(1, 1), padding='valid', activate=self.activate)\r\n","        net = self.conv2d_bn(net, 64, 3, 3, strides=(1, 1), activate=self.activate)\r\n","        branch_0 = MaxPooling2D((3, 3), strides=(2, 2), padding='valid')(net)\r\n","        net = self.conv2d_bn(branch_0, 80, 3, 3, strides=(2, 2), padding='valid', activate=self.activate)\r\n","        net = self.conv2d_bn(net, 192, 3, 3, strides=(1, 1), padding='valid', activate=self.activate)\r\n","        x = MaxPooling2D((3, 3), strides=(2, 2), padding=\"valid\")(net)\r\n","\r\n","        # inception1\r\n","        branch_0 = self.conv2d_bn(x, 96, 1, 1, strides=(1, 1), activate=self.activate)\r\n","        branch_1 = self.conv2d_bn(x, 64, 1, 1, strides=(1, 1), activate=self.activate)\r\n","        branch_1 = self.conv2d_bn(branch_1, 96, 3, 3, strides=(1, 1), activate=self.activate)\r\n","        branch_2 = self.conv2d_bn(x, 64, 1, 1, strides=(1, 1), activate=self.activate)\r\n","        branch_2 = self.conv2d_bn(branch_2, 96, 3, 3, strides=(1, 1), activate=self.activate)\r\n","        branch_2 = self.conv2d_bn(branch_2, 96, 3, 3, strides=(1, 1), activate=self.activate)\r\n","        x = [branch_0, branch_1, branch_2]\r\n","        mix1 = concatenate(x, axis=channel_axis)\r\n","        x = self.conv2d_bn(mix1, 96, 1, 1, strides=(1, 1), padding='valid', activate=self.activate)\r\n","\r\n","        # inception2\r\n","        branch_0 = self.conv2d_bn(x, 64, 3, 3, strides=(1, 1), activate=self.activate)\r\n","        branch_1 = self.conv2d_bn(x, 96, 1, 1, strides=(1, 1), activate=self.activate)\r\n","        branch_1 = self.conv2d_bn(branch_1, 128, 3, 3, strides=(1, 1), activate=self.activate)\r\n","        branch_1 = self.conv2d_bn(branch_1, 160, 3, 3, strides=(1, 1), activate=self.activate)\r\n","        branch_3 = AveragePooling2D((3, 3), strides=(1, 1), padding='same', name=\"avg_pool_1\")(x)\r\n","        if (self.use_dense_block):\r\n","            x1 = [x, branch_0, branch_1, branch_3]\r\n","        else:\r\n","            x1 = [branch_0, branch_1, branch_3]\r\n","        mix2 = concatenate(x1, axis=channel_axis)\r\n","\r\n","        # inception3\r\n","        branch_0 = self.conv2d_bn(mix2, 192, 1, 1, strides=(1, 1), activate=self.activate)\r\n","        branch_1 = self.conv2d_bn(mix2, 128, 1, 1, strides=(1, 1), activate=self.activate)\r\n","        branch_1 = self.conv2d_bn(branch_1, 160, 1, 7, strides=(1, 1), activate=self.activate)\r\n","        branch_1 = self.conv2d_bn(branch_1, 160, 7, 1, strides=(1, 1), activate=self.activate)\r\n","        if (self.use_dense_block):\r\n","            x = [x, mix2, branch_0, branch_1]\r\n","        else:\r\n","            x = [mix2, branch_0, branch_1]\r\n","        mix3 = concatenate(x, axis=channel_axis, name='mixed3')\r\n","\r\n","        # translate layer\r\n","        if (self.use_dense_block):\r\n","            x = self.conv2d_bn(mix3, 192, 1, 1, strides=(1, 1), padding='valid', activate=self.activate)\r\n","            x = AveragePooling2D((2, 2), strides=(2, 2))(x)\r\n","            x1 = BatchNormalization(scale=True, axis=channel_axis)(x)\r\n","        else:\r\n","            x1 = self.conv2d_bn(mix3, 192, 1, 1, strides=(1, 1), padding='valid', activate=self.activate)\r\n","\r\n","        # inception4\r\n","        netb00 = self.conv2d_bn(x1, 192, 1, 1, strides=(1, 1), padding='same', activate=self.activate)\r\n","        netb10 = self.conv2d_bn(x1, 192, 1, 1, strides=(1, 1), padding='same', activate=self.activate)\r\n","        netb11 = self.conv2d_bn(netb10, 256, 3, 3, strides=(1, 1), padding='same', activate=self.activate)\r\n","        netb20 = self.conv2d_bn(x1, 160, 1, 1, strides=(1, 1), padding='same', activate=self.activate)\r\n","        netb21 = self.conv2d_bn(netb20, 192, 3, 3, strides=(1, 1), padding='same', activate=self.activate)\r\n","        netb22 = self.conv2d_bn(netb21, 256, 3, 3, strides=(1, 1), padding='same', activate=self.activate)\r\n","        netb30 = AveragePooling2D((3, 3), strides=(1, 1), padding='same')(x1)\r\n","        netb31 = self.conv2d_bn(netb30, 160, 1, 1, strides=(1, 1), padding='same', activate=self.activate)\r\n","        if (self.use_dense_block):\r\n","            x = concatenate([x, netb00, netb11, netb22, netb31], axis=channel_axis, name='mixed4')\r\n","        else:\r\n","            x = concatenate([netb00, netb11, netb22, netb31], axis=channel_axis, name='mixed4')\r\n","\r\n","        # inception5 * 2\r\n","        feature_list = [x]\r\n","        for _ in range(2):\r\n","            branch_0 = self.conv2d_bn(x, 256, 1, 1, strides=(1, 1), activate=self.activate)\r\n","            branch_1 = self.conv2d_bn(x, 128, 1, 3, strides=(1, 1), activate=self.activate)\r\n","            branch_1 = self.conv2d_bn(branch_1, 192, 3, 1, strides=(1, 1), activate=self.activate)\r\n","            branch_1 = self.conv2d_bn(branch_1, 256, 1, 3, strides=(1, 1), activate=self.activate)\r\n","            a = [branch_0, branch_1]\r\n","            mix5 = concatenate(a, axis=channel_axis)\r\n","            x1 = self.conv2d_bn(mix5, 256, 1, 1, strides=(1, 1), padding='valid', activate=self.activate)\r\n","            x = concatenate([x, x1], axis=channel_axis)\r\n","            feature_list.append(x)\r\n","        if (self.use_global_average_pool):\r\n","            x = concatenate(feature_list, axis=channel_axis)\r\n","\r\n","        if (self.use_global_average_pool):\r\n","            # GlobalAveragePooling Layer\r\n","            x = GlobalAveragePooling2D(name='global_avg_pool')(x)\r\n","        else:\r\n","            # Fully Connection Layer\r\n","            x = Dense(2000)(x)\r\n","            x = Dense(1000)(x)\r\n","        x = Dense(7, name='Logits')(x)\r\n","        x = Activation('softmax', name='probb')(x)\r\n","        model = Model(inputs=input_shape, outputs=x, name='ANet')\r\n","        return model"],"execution_count":3,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"rFCeR87ektOq"},"source":["Training.py 訓練"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":543},"id":"acc1zf5Lj0HF","executionInfo":{"status":"error","timestamp":1611799555997,"user_tz":-480,"elapsed":43744,"user":{"displayName":"吳貞宜","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhIOzgYhbporG1Eax-zD0HNdC2UZr8CRTfHsZcY5g=s64","userId":"00889427696288622203"}},"outputId":"ef6f61f0-3da9-405f-b8f4-364f47c5fc5c"},"source":["\r\n","\r\n","\r\n","\r\n","if __name__=='__main__':\r\n","    ## 資料\r\n","    #   1. 建立資料\r\n","    data = Data(filename='JAFFE', data_type=DataType.ALL_DATA) #10/13要改\r\n","    ## 模型\r\n","    #   1. 建立模型類別\r\n","    custom_model = Dense_FaceLiveNet()\r\n","    #   2. 設定模型\r\n","    custom_model.set_model(load_model('/content/drive/MyDrive/basic_emotion/Dense_FaceLiveNet.hdf5'))#此模型dense為7\r\n","    #   3. 遷移學習\r\n","    custom_model.transfer_learning(custom_model.get_model())\r\n","    #   4. 設定模型的 loss function 與 最佳化方法\r\n","    custom_model.model_compile()\r\n","    #   5. 取得模型\r\n","    model = custom_model.get_model()\r\n","    # 訓練器\r\n","    #   1. 建立訓練器\r\n","    trainer = Trainer(data=data, model=model)\r\n","\r\n","    #   2. 設定 callback function\r\n","    trainer.set_callbacks(filename='gaga')\r\n","    # #   3. 開始訓練\r\n","    trainer.fit(filename='gaga')\r\n","\r\n","    #from sklearn.model_selection import StratifiedKFold\r\n","    # #\r\n","     #skf = StratifiedKFold(n_splits=5, shuffle=True)\r\n","     #X = np.load('D:\\\\NianXiang_File\\\\Research\\\\Dataset\\\\s_data_x.npy')\r\n","     #X = Data.static_arr_reshape(arr=X)\r\n","     #y = np.load('D:\\\\NianXiang_File\\\\Research\\\\Dataset\\\\s_data_y.npy')\r\n","    # # # CNN.epochs=200\r\n","    #for index, (train_indices, val_indices) in enumerate(skf.split(X, y)):\r\n","         #xtrain, xval = X[train_indices], X[val_indices]\r\n","         #ytrain, yval = y[train_indices], y[val_indices]\r\n","         #ytrain = np_utils.to_categorical(ytrain, 6)\r\n","    #     yval = np_utils.to_categorical(yval, 6)\r\n","    #\r\n","    #     check_point = ModelCheckpoint(\"callbacks.{epoch:02d}-{val_acc:.5f}.hdf5\", monitor='val_acc',\r\n","    #                                    verbose=1,\r\n","    #                                    save_best_only=True, mode='max')\r\n","    #     earlyStopping = EarlyStopping(monitor='val_acc', min_delta=0.001, patience=20, verbose=2,\r\n","    #                                    mode='max')\r\n","    #     reduce_lr = ReduceLROnPlateau(monitor='val_acc', factor=0.2, patience=5, min_lr=1E-8)\r\n","    #\r\n","    #     model = None\r\n","    #     custom_model = CustomModel()\r\n","    #     #   2. 設定模型\r\n","    #     custom_model.set_model(load_model('D:\\\\NianXiang_File\\\\Research\\\\Model\\\\Dense_FaceLiveNet.hdf5'))\r\n","    #     #   3. 遷移學習\r\n","    #     custom_model.transfer_learning(custom_model.get_model())\r\n","    #     #   4. 設定模型的 loss function 與 最佳化方法\r\n","    #     custom_model.model_compile()\r\n","    #     #   5. 取得模型\r\n","    #     model = custom_model.get_model()\r\n","    #     model.compile(loss='categorical_crossentropy', optimizer='adamax', metrics=['acc'])\r\n","    #     history = model.fit(x=xtrain, y=ytrain, validation_data=(xval, yval),\r\n","    #                         callbacks=[check_point, earlyStopping, reduce_lr],\r\n","    #                         epochs=200)\r\n","    #     pd.DataFrame(history.history).to_csv('Dense_FaceLiveNet_fold{}_history.csv'.format(index))\r\n","    #     del model\r\n","    #     gc.collect()"],"execution_count":4,"outputs":[{"output_type":"stream","text":["- create data instance\n","\t- set data type ... ALL_DATA\n","\t- set X and Y ... OK\n","\t- set image shape ... OK\n","\t- start data pre process ... OK\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py:1844: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n","  warnings.warn('`Model.fit_generator` is deprecated and '\n"],"name":"stderr"},{"output_type":"stream","text":["Epoch 1/50\n","  1/220 [..............................] - ETA: 1:17:17 - loss: 2.2066 - acc: 0.1146"],"name":"stdout"},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-4-ad3669557208>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_callbacks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'gaga'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m     \u001b[0;31m# #   3. 開始訓練\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m     \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'gaga'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m     \u001b[0;31m#from sklearn.model_selection import StratifiedKFold\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-2-322184b141ad>\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, argument_amount, per_epoch_amount, filename, use_steps)\u001b[0m\n\u001b[1;32m     65\u001b[0m                                                 \u001b[0;31m# validation_steps=self.batch_size,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m                                                 \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m                                                 \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m                                                 \u001b[0;31m# class_weight=self.class_weight\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m                                                 )\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, validation_freq, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m   1859\u001b[0m         \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1860\u001b[0m         \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1861\u001b[0;31m         initial_epoch=initial_epoch)\n\u001b[0m\u001b[1;32m   1862\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1863\u001b[0m   def evaluate_generator(self,\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1098\u001b[0m                 _r=1):\n\u001b[1;32m   1099\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1100\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1101\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1102\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    826\u001b[0m     \u001b[0mtracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    827\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtrace\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTrace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_name\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtm\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 828\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    829\u001b[0m       \u001b[0mcompiler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"xla\"\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_experimental_compile\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"nonXla\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    830\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    853\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    854\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 855\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    856\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    857\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2941\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[1;32m   2942\u001b[0m     return graph_function._call_flat(\n\u001b[0;32m-> 2943\u001b[0;31m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[0m\u001b[1;32m   2944\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2945\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1917\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1918\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1919\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1920\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1921\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    558\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    559\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 560\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    561\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    562\u001b[0m           outputs = execute.execute_with_cancellation(\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0;32m---> 60\u001b[0;31m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]}]}