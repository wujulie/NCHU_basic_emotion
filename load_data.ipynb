{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"load_data.ipynb","provenance":[],"authorship_tag":"ABX9TyMRY2MkoJeo+y4rk9uiYoSz"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"code","metadata":{"id":"cMhg6qY4Clbn"},"source":["import dlib\r\n","import time\r\n","import os\r\n","\r\n","from PIL import Image\r\n","from keras.layers import Activation,K\r\n","from keras.models import load_model\r\n","from keras.utils import plot_model, get_custom_objects, np_utils\r\n","import cv2\r\n","import numpy as np\r\n","from sklearn.metrics import classification_report,confusion_matrix\r\n","import gc\r\n","import itertools\r\n","import numpy as np\r\n","import matplotlib.pyplot as plt\r\n","def cal_acc(name, accuracy):\r\n","    print(\"{} Accuracy = {:.2f}% - (+/- {:.2f}%)\".format(name, np.mean(accuracy), np.std(accuracy)))\r\n","\r\n","def get_data_from_path(path = 'D:\\\\NianXiang_File\\\\CNN\\\\Dataset\\\\LE', file_name='BE'):\r\n","    # data = np.load('predict_y_nor.npy')\r\n","    output = {}\r\n","    x = []\r\n","    y = []\r\n","    folders = []\r\n","    for root, dirs, files in os.walk(path, topdown=False):\r\n","        for name in dirs:\r\n","            folders.append(name)\r\n","    folders = folders[-7:]\r\n","    start_time = time.time()\r\n","    count = 0\r\n","    for fld in folders:\r\n","        index = folders.index(fld)\r\n","        print('Loading {} files (Index: {})'.format(fld, index))\r\n","        for root, dirs, files in os.walk(path + '\\\\' + fld, topdown=False):\r\n","            for file in files:\r\n","                print(file)\r\n","                if (file == '1.jpg'):\r\n","                    pass\r\n","                else:\r\n","                    filepath = root + '\\\\' + file\r\n","                    detector = dlib.get_frontal_face_detector()\r\n","                    img = cv2.imread(filepath)\r\n","                    face = detector(img, 1)\r\n","                    for i, d in enumerate(face):\r\n","                        x1 = d.left()\r\n","                        y1 = d.top()\r\n","                        x2 = d.right()\r\n","                        y2 = d.bottom()\r\n","                        img = img[y1:y2, x1:x2]\r\n","                    lpath = 'D:\\\\NianXiang_File\\\\CNN\\\\Dataset\\\\small_dataset\\\\NEW'\r\n","                    filename = lpath + '\\\\' + str(index) + '\\\\' + str(count) + '.jpg'\r\n","                    cv2.imwrite(filename, img)\r\n","                    try:\r\n","                        img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\r\n","                        resized = cv2.resize(img, (224, 224), cv2.INTER_LANCZOS4)\r\n","                        y.append(index)\r\n","                        x.append(resized)\r\n","                        count = count + 1\r\n","                    except:\r\n","                        pass\r\n","\r\n","                # file = Data.static_load_image(ffile)\r\n","    np.save('{}_X.npy'.format(file_name), x)\r\n","    np.save('{}_Y.npy'.format(file_name), y)\r\n","    print(\"Complete\")\r\n","\r\n","def confusion_matri(model=None, normalize=False, title='Confusion Matrix', cls='be', x=None, y=None):\r\n","    predict = model.predict(x=x, batch_size=16, verbose=2)\r\n","    pred = np.argmax(predict, axis=1)\r\n","    loss, score = model.evaluate(x=x, y=y, batch_size=16, verbose=2)\r\n","\r\n","    cnf_matrix = confusion_matrix(np.argmax(y, axis=1), pred)\r\n","    np.set_printoptions(precision=2)\r\n","    plt.figure()\r\n","    if cls == 'be':\r\n","        class_name = ['Angry', 'Disgust', 'Fear', 'Happy', 'Sad', 'Surprise', 'Neutral']\r\n","    if cls == 'le':\r\n","        class_name = [\"Frustration\", \"Confused\", \"Bored\", \"Delightful\", \"Flow\", \"Surprise\"]\r\n","    __plot_confusion_matrix(cnf_matrix, classes=class_name,\r\n","                            title=title,\r\n","                            normalize=normalize)\r\n","    print('{:.2f}%'.format(score * 100))\r\n","    plt.show()\r\n","    # print(\"Accuracy：{:.2f}%\".format(score * 100))\r\n","\r\n","def __plot_confusion_matrix(cm, classes,\r\n","                            normalize=False,\r\n","                            title='Confusion matrix',\r\n","                            cmap=plt.cm.Blues):\r\n","    \"\"\"\r\n","    This function prints and plots the confusion matrix.\r\n","    Normalization can be applied by setting `normalize=True`.\r\n","    \"\"\"\r\n","    if normalize:\r\n","        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\r\n","        print(\"Normalized confusion matrix\")\r\n","    else:\r\n","        print('Confusion matrix, without normalization')\r\n","    print(cm)\r\n","\r\n","    plt.imshow(cm, interpolation='nearest', cmap=cmap)\r\n","    plt.title(title)\r\n","    plt.colorbar()\r\n","    tick_marks = np.arange(len(classes))\r\n","    plt.xticks(tick_marks, classes, rotation=45)\r\n","    plt.yticks(tick_marks, classes)\r\n","\r\n","    fmt = '.2f' if normalize else 'd'\r\n","    thresh = cm.max() / 2.\r\n","    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\r\n","        plt.text(j, i, format(cm[i, j], fmt),\r\n","                 horizontalalignment=\"center\",\r\n","                 color=\"white\" if cm[i, j] > thresh else \"black\")\r\n","\r\n","    plt.tight_layout()\r\n","    plt.ylabel('True label')\r\n","    plt.xlabel('Predicted label')\r\n","if __name__==\"__main__\":\r\n","    def swish(x):\r\n","        return K.sigmoid(x) * x\r\n","\r\n","\r\n","    class Swish(Activation):\r\n","        def __init__(self, activation, **kwargs):\r\n","            super(Swish, self).__init__(activation, **kwargs)\r\n","            self.__name__ = 'swish'\r\n","    get_custom_objects().update({\"swish\": Swish(swish)})\r\n","    #\r\n","    # path = \"D:\\\\NianXiang_File\\\\CNN\\\\Dataset\\\\mix\\\\\"\r\n","    #\r\n","    model = load_model('E:\\\\交接\\\\1_賴念祥code\\\\賴念祥code\\\\Researchyes\\\\Research\\\\Train\\\\callbacks.26-0.84091.hdf5')\r\n","\r\n","    # for i in range(7):\r\n","    #     x = np.load('C_FRONT_X_{}.npy'.format(i))\r\n","    #     # y = np_utils.to_categorical(y, 7).astype('int')\r\n","    #     x = x.reshape(x.shape[0], 224, 224, 1)\r\n","    #     predict = model.predict(x=x, batch_size=16, verbose=2)\r\n","    #     pred = np.argmax(predict, axis=1)\r\n","    #     print(\"{} - {}\".format(i, pred))\r\n","    #\r\n","    #     for index, img in enumerate(x):\r\n","    #         single_path = path + \"{}\".format(pred[index])\r\n","    #         if not os.path.exists(single_path):\r\n","    #             os.makedirs(single_path)\r\n","    #         img = cv2.imwrite(single_path + '\\\\{}.jpg'.format(index), img)\r\n","    # loss, score = model.evaluate(x=x, y=y, batch_size=16, verbose=2)\r\n","    # x = np.load('data/Model2_X.npy')\r\n","    # x = np.load('DATA_TEST_X.npy')\r\n","    # y = np.load('data/Model2_Y.npy')\r\n","    # x = np.load('C_FINAL_X.npy')\r\n","    # y = np.load('DATA_TEST_Y.npy')\r\n","    # print(np.unique(y))\r\n","    x = np.load('E:\\\\交接\\\\1_賴念祥code\\\\賴念祥code\\\\Researchyes\\\\Research\\\\Dataset\\\\s_data_x.npy')\r\n","    y = np.load('E:\\\\交接\\\\1_賴念祥code\\\\賴念祥code\\\\Researchyes\\\\Research\\\\Dataset\\\\s_data_y.npy')\r\n","    y = np_utils.to_categorical(y, 6).astype('int')\r\n","    x = x.reshape(x.shape[0], 224, 224, 1)\r\n","    print(x.shape, y.shape)\r\n","    # loss, score = model.evaluate(x=x, y=y, batch_size=16, verbose=2)\r\n","    # print(score)\r\n","\r\n","    # pred = model.predict(x=x,batch_size=64,verbose=1)\r\n","    # pred = np.argmax(pred, axis=1)\r\n","    # print(pred.shape)\r\n","    confusion_matri(model=model, normalize=False,\r\n","                    title=\"Learning Emotion\",\r\n","                    cls='le',x=x,y=y)\r\n","        # for index, data in enumerate(predict):\r\n","        # predict[index] = np.argmax(data)\r\n","    # np.save('outcome.npy', predict)\r\n","\r\n","    # model = load_model('D:\\\\NianXiang_File\\\\CNN\\\\Outcome\\\\LearningEmotion\\\\NXNet\\\\transfer learning\\\\NXNet_LE.17-0.85572.hdf5')\r\n","    # x1 = np.load('4106029006_X.npy')\r\n","    # x2 = np.load('4104030407_X.npy')\r\n","    # x3 = np.load('4103044033_X.npy')\r\n","    # x1 = np.load('C_X.npy')\r\n","    # y1 = np.load('C_Y.npy')\r\n","    # x2 = np.load('F_X.npy')\r\n","    # y2 = np.load('F_Y.npy')\r\n","    # x3 = np.load('NEW_LE_X.npy')\r\n","    # y3 = np.load('NEW_LE_Y.npy')\r\n","    # X = np.load('combine_le_x1.npy')\r\n","    # Y = np.load('combine_le_y1.npy')\r\n","    # x = np.load('KDEF_x.npy')\r\n","    # y = np.load(\"KDEF_y.npy\")\r\n","    # _x = []\r\n","    # _y = []\r\n","    # for index, label in enumerate(y):\r\n","    #     if label == 3:\r\n","    #        _x.append(x[index])\r\n","    #        _y.append(label)\r\n","    # _x = np.array(_x)\r\n","    # _y = np.array(_y)\r\n","    # Y = np.array([Y, _y])\r\n","    # yy = []\r\n","    # for i in Y:\r\n","    #     for data in i:\r\n","    #       yy.append(data)\r\n","    # Y = np.array(yy)\r\n","    #\r\n","    # X = np.vstack((X, _x))\r\n","    #\r\n","    # print(X.shape, Y.shape)\r\n","    # np.save('combine_le_argument_x.npy', X)\r\n","    # np.save('combine_le_argument_y.npy', Y)\r\n","    # y = np.load('KDEF_y.npy')\r\n","    # count0 ,count1, count2, count3, count4 ,count5 ,count6 = 0,0,0,0,0,0,0\r\n","    # for data in y:\r\n","    #     data = int(data)\r\n","    #     if data == 0:\r\n","    #         count0 += 1\r\n","    #     if data == 1:\r\n","    #         count1 += 1\r\n","    #     if data == 2:\r\n","    #         count2 += 1\r\n","    #     if data == 3:\r\n","    #         count3 += 1\r\n","    #     if data == 4:\r\n","    #         count4 += 1\r\n","    #     if data == 5:\r\n","    #         count5 += 1\r\n","    #     if data == 6:\r\n","    #         count6 += 1\r\n","    # print(count0 ,count1, count2, count3, count4 ,count5 ,count6)\r\n","    # x = np.vstack((x1, x2))\r\n","    # Y = np.array([y1, y2 ])\r\n","    # y = []\r\n","    # for i in Y:\r\n","    #     for data in i:\r\n","    #       y.append(data)\r\n","    # y = np.array(y)\r\n","    # # y = np.vstack((y, y1))\r\n","    # np.save('combine_le_x1.npy', x)\r\n","    # np.save('combine_le_y1.npy', y)\r\n","\r\n","    # x = x.reshape(x.shape[0], 224, 224, 1)\r\n","    # y = np_utils.to_categorical(y, 7).astype('int')\r\n","    # confusion_matri(model=model, normalize=False, title=\"Learning Emotion\",cls='le',x=x,y=y)\r\n","    # datas = []\r\n","    # for data in output:\r\n","    #     datas.append(np.argmax(data))\r\n","    # np.save('0_Y.npy'.format(), datas)\r\n","    # input_list = [x1, x2, x3, x4]\r\n","    # for index, x in enumerate(input_list):\r\n","    #     x = x.reshape(x.shape[0], 224, 224, 1)\r\n","    #     output = model.predict(x=x, batch_size=64, verbose=2)\r\n","    #     datas = []\r\n","    #     for data in output:\r\n","    #         datas.append(np.argmax(data))\r\n","    #     np.save('{}_Y.npy'.format(index), datas)\r\n","    # np.save('output.npy', y)\r\n","\r\n","\r\n","\r\n","    # folders = []\r\n","    # path = 'D:\\\\NianXiang_File\\\\CNN\\\\Dataset\\\\LE_Test\\\\Final'\r\n","    # for root, dirs, files in os.walk(path, topdown=False):\r\n","    #     for name in dirs:\r\n","    #         folders.append(name)\r\n","    # folders = folders[-7:]\r\n","    # y_list = [2,1,0,3]\r\n","    # x = []\r\n","    # start_time = time.time()\r\n","    # count = 0\r\n","    # y = np.load('outcome.npy')\r\n","    # for fld in folders:\r\n","    #     index = folders.index(fld)\r\n","    #     y = np.load('{}_Y.npy'.format(index))\r\n","    #     print('Loading {} files (Index: {})'.format(fld, index))\r\n","    #     fold = os.path.basename(fld)\r\n","    #     p = os.path.join(path, fold, '*g')\r\n","    #     files = glob.glob(p)\r\n","    #     output = []\r\n","    #     x = []\r\n","    #     for i, fl in enumerate(files):\r\n","    #         flbase = os.path.basename(fl)\r\n","    #         data = np.array([str(flbase).split('.')[0], y[i]])\r\n","    #         output.append(data)\r\n","    #     import pandas as pd\r\n","    #     p = pd.DataFrame(data=output, columns=['file', 'class'])\r\n","    #     p.to_csv('{}.csv'.format(fld))\r\n","\r\n","    #         img = cv2.imread(fl)\r\n","    #         img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\r\n","    #         data_x = cv2.resize(img, (224, 224), cv2.INTER_LANCZOS4)\r\n","    #         x.append(data_x)\r\n","    #     np.save('{}_X.npy'.format(fld), x)\r\n","    #     x = []\r\n","\r\n","\r\n","    # np.save('NEW_LE_Y.npy', y)\r\n","    # get_data_from_path(path='D:\\\\NianXiang_File\\\\CNN\\\\Dataset\\\\small_dataset\\\\ORIGIN', file_name='small_data')\r\n","    # y = np.load('new_le_train_y.npy')\r\n","    # print(len(y))\r\n","    # # x = np.load(\"NEW_LE_X.npy\")\r\n","    # y = np.load(\"NEW_LE_Y.npy\")\r\n","    # print(len(y))\r\n","    # X = []\r\n","    # Y = []\r\n","    # for i in range(len(y)):\r\n","    #     if y[i] == 4 or y[i] == 6:\r\n","    #         x[i] = np.zeros((224,224))\r\n","    #     X.append(x[i])\r\n","    #     Y.append(y[i])\r\n","    # np.save('NEW_LE_X_1.npy', X)\r\n","    # np.save('NEW_LE_Y_1.npy', Y)\r\n","\r\n","    # path = 'D:\\\\NianXiang_File\\\\CNN\\\\Dataset\\\\NEW_LE\\\\'\r\n","    # index = 0\r\n","    # for i in range(len(y)):\r\n","    #     filename = path + str(y[i]) + '\\\\' + str(index) + '.jpg'\r\n","    #     cv2.imwrite(filename, x[i])\r\n","    #     print(filename)\r\n","    #     index += 1\r\n","    # import tensorflow as tf\r\n","    # import keras.backend.tensorflow_backend as KTF\r\n","    #\r\n","    # KTF.set_session(tf.Session(config=tf.ConfigProto(device_count={'gpu': 0})))\r\n","    # model = load_model('NXNet_fold3_LE.11-0.79500.hdf5')\r\n","    # plot_model(model, to_file='NXNet.png')\r\n","    # cal_acc('DenseNet', [97.77, 97.67, 95.23, 95.23, 90.24])\r\n","    # cal_acc('Inception-ResNetV2', [92.68, 90.47, 78.57, 88.37, 97.77])\r\n","    # cal_acc('InceptionV3', [86.66, 79.07, 78.57, 85.71, 82.92])\r\n","    # cal_acc('VGG', [82.22, 93.02, 78.57, 85.71, 92.68])\r\n","    # cal_acc('NXNet', [95.55, 95.34, 90.47, 85.71, 87.80])\r\n","    #\r\n","    # # x1 = np.load('JAFFE_x.npy')\r\n","    # x2 = np.load('KDEF_x.npy')\r\n","    # y1 = np.load('JAFFE_y.npy')\r\n","    # y2 = np.load('KDEF_y.npy')\r\n","    # x = np.concatenate((x1, x2), axis=0)\r\n","    # y = np.concatenate((y1, y2), axis=0)\r\n","    # np.save('KDEF_JAFFE_x.npy', x)\r\n","    # np.save('KDEF_JAFFE_y.npy', y)\r\n","    # print('have {} data'.format(len(y)))\r\n","    # Dataset.load_jaffe()\r\n","    # Dataset.load_KDEF_dataset()"],"execution_count":null,"outputs":[]}]}