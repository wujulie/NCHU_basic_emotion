{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"predict.ipynb","provenance":[],"mount_file_id":"1cV-Jjrb4xI6Ktm8ihefDLuHgHMo4S4VZ","authorship_tag":"ABX9TyM8dZWHqt2OjIBRE2Qg3vFd"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"ZfS31Vx5knER"},"source":["Data.py 資料"]},{"cell_type":"code","metadata":{"id":"mqm2XpA33kEf","executionInfo":{"status":"ok","timestamp":1611802043513,"user_tz":-480,"elapsed":4268,"user":{"displayName":"吳貞宜","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhIOzgYhbporG1Eax-zD0HNdC2UZr8CRTfHsZcY5g=s64","userId":"00889427696288622203"}}},"source":["import itertools\r\n","import cv2\r\n","import dlib\r\n","import numpy as np\r\n","import matplotlib.pyplot as plt\r\n","import pandas as pd\r\n","import time\r\n","from keras.preprocessing.image import ImageDataGenerator\r\n","from keras.utils import np_utils\r\n","from sklearn.model_selection import train_test_split\r\n","from sklearn.metrics import confusion_matrix\r\n","from sklearn.preprocessing import StandardScaler\r\n","from enum import Enum\r\n","# 輸入資料型態\r\n","from sklearn.utils import shuffle\r\n","import os\r\n","# 資料類別\r\n","class DataType(Enum):\r\n","    ALL_DATA = 'all data'       # 所有資料\r\n","    TRAIN = 'train data'        # 只有訓練資料\r\n","    TEST = 'test data'          # 只有測試資料\r\n","    CUSTOM = 'custom data'      # 原先設計好之資料\r\n","    SMALL = 'small dataset'     # 小型資料集\r\n","\r\n","# 輸入資料\r\n","class Data(object):\r\n","    def __init__(self, data_type=DataType.ALL_DATA,\r\n","                 img_width=224, img_height=224,\r\n","                 split_data=False, filename='small_data', num_classes=7,#訓練時須修改標籤數目\r\n","                 data_generator=True):\r\n","        print(\"- create data instance\")\r\n","        try:\r\n","            print(\"\\t- set data type ... \", end=\"\")\r\n","            self.data_type=data_type\r\n","            self.num_classes=num_classes\r\n","            self.filename = filename\r\n","            print(\"{}\".format(data_type.name))\r\n","        except:\r\n","            print(\"\")\r\n","        try:\r\n","            print('\\t- set X and Y ... ', end=\"\")\r\n","            self.x = np.load('/content/drive/MyDrive/basic_emotion/' + filename + '_x.npy')\r\n","            self.y = np.load('/content/drive/MyDrive/basic_emotion/' + filename + '_y.npy')\r\n","            print(\"OK\")\r\n","        except  Exception as e:\r\n","            print(e)\r\n","        try:\r\n","            print(\"\\t- set image shape ... \", end=\"\")\r\n","            self.img_width=img_width\r\n","            self.img_height=img_height\r\n","            print(\"OK\")\r\n","        except Exception as e:\r\n","            print(\"image shape setting error\")\r\n","            print(e)\r\n","        self.split_data=split_data\r\n","        try:\r\n","            print(\"\\t- start data pre process ... \", end=\"\")\r\n","            self.data_generator = data_generator\r\n","            self.pre_process()\r\n","            print(\"OK\")\r\n","        except  Exception as e:\r\n","            print(\"error\")\r\n","            print(e)\r\n","\r\n","\r\n","    # 資料預處理\r\n","    def pre_process(self, test_size=0.2, val_size=0.1, filename=None):\r\n","        x, y = shuffle(self.x, self.y)\r\n","        x = x.reshape(x.shape[0], self.img_width, self.img_height, 1)\r\n","        if (self.data_type == DataType.ALL_DATA):\r\n","            train_x, test_x, train_y, test_y = train_test_split(x, y, test_size=test_size, random_state=25)\r\n","            train_x, val_x, train_y, val_y = train_test_split(x, y, test_size=val_size, random_state=25)\r\n","        if (self.data_type == DataType.TRAIN):\r\n","            train_x, test_x, train_y, test_y = train_test_split(x, y, test_size=test_size, random_state=25)\r\n","            val_x = test_x\r\n","            val_y = test_y\r\n","        if (self.data_type == DataType.TEST or self.data_type == DataType.SMALL):\r\n","            train_x = x\r\n","            train_y = y\r\n","            test_x = x\r\n","            test_y = y\r\n","            val_x = x\r\n","            val_y = y\r\n","\r\n","        if (self.data_type == DataType.CUSTOM):\r\n","            self.train_x = self.arr_reshape(x)\r\n","            self.train_y = y\r\n","            self.test_x, self.test_y = shuffle(np.load('/content/drive/MyDrive/basic_emotion/{}_test_x.npy'.format(filename)), np.load('/content/drive/MyDrive/basic_emotion/{}_test_y.npy'.format(filename)))\r\n","            self.val_x, self.val_y = shuffle(np.load('/content/drive/MyDrive/basic_emotion/{}_val_x.npy'.format(filename)), np.load('/content/drive/MyDrive/basic_emotion/{}_val_y.npy'.format(filename)))\r\n","            self.test_x = self.arr_reshape(self.test_x)\r\n","            self.val_x = self.arr_reshape(self.val_x)\r\n","\r\n","        self.train_x = train_x\r\n","        self.train_y = train_y\r\n","        self.val_x = val_x\r\n","        self.val_y = val_y\r\n","        self.test_x = test_x\r\n","        self.test_y = test_y\r\n","        self.one_hot_encoding()\r\n","        self.save_data()\r\n","\r\n","    # 對輸出值進行 one hot encodcing\r\n","    def one_hot_encoding(self):\r\n","        try:\r\n","            pass\r\n","            self.train_y = np_utils.to_categorical(self.train_y, self.num_classes).astype('int')\r\n","            self.test_y = np_utils.to_categorical(self.test_y, self.num_classes).astype('int')\r\n","            self.val_y = np_utils.to_categorical(self.val_y, self.num_classes).astype('int')\r\n","        except  Exception as e:\r\n","            print(e)\r\n","\r\n","    # 對 numpy 進行矩陣重組\r\n","    def arr_reshape(self, arr):\r\n","        return arr.reshape(arr.shape[0], self.img_width, self.img_height, 1)\r\n","\r\n","    # 靜態取用矩陣重組\r\n","    @classmethod\r\n","    def static_arr_reshape(self, arr, img_width=224, img_height=224):\r\n","        return arr.reshape(arr.shape[0], img_width, img_height, 1)\r\n","\r\n","    # 靜態取用 one hot encoding\r\n","    @classmethod\r\n","    def static_one_hot_encoding(self, y=None, classes=6):\r\n","        return np_utils.to_categorical(y, classes).astype('int')\r\n","\r\n","    # 資料擴增\r\n","    def generator(self, batch_size):\r\n","        # train_datagen = ImageDataGenerator(horizontal_flip=True, rotation_range=10, zoom_range=0.2)\r\n","        # train_datagen = ImageDataGenerator(horizontal_flip=True, rotation_range=10)\r\n","        if self.data_generator:\r\n","            train_datagen = ImageDataGenerator(horizontal_flip=True, rotation_range=10, zoom_range=0.2)\r\n","            # train_datagen = ImageDataGenerator(horizontal_flip=True, rotation_range=10)\r\n","            train_datagen.fit(self.train_x)\r\n","            train_generator = train_datagen.flow(self.train_x, self.train_y, batch_size=batch_size)\r\n","            return train_generator\r\n","        else:\r\n","            train_datagen = ImageDataGenerator()\r\n","            train_datagen.fit(self.train_x)\r\n","            train_generator = train_datagen.flow(self.train_x, self.train_y, batch_size=batch_size)\r\n","            return train_generator\r\n","\r\n","    # 資料儲存\r\n","    def save_data(self):\r\n","        np.save('/content/drive/MyDrive/basic_emotion/' + self.filename + '_train_x.npy', self.train_x)#10/13改過\r\n","        np.save('/content/drive/MyDrive/basic_emotion/' + self.filename + '_train_y.npy', self.train_y)\r\n","        np.save('/content/drive/MyDrive/basic_emotion/' + self.filename + '_val_x.npy', self.val_x)\r\n","        np.save('/content/drive/MyDrive/basic_emotion/' + self.filename + '_val_y.npy', self.val_y)\r\n","        np.save('/content/drive/MyDrive/basic_emotion/' + self.filename + '_test_x.npy', self.test_x)\r\n","        np.save('/content/drive/MyDrive/basic_emotion/' + self.filename + '_test_y.npy', self.test_y)\r\n","\r\n","    # 將圖片轉換成矩陣\r\n","    @classmethod\r\n","    def load_data(self, path='', filename=''):\r\n","        folders, x, y= [], [], []\r\n","        for root, dirs, files in os.walk(path, topdown=False):\r\n","            for name in dirs:\r\n","                folders.append(name)\r\n","        folders = folders[-7:]\r\n","        start_time = time.time()\r\n","        count = 0\r\n","        for fld in folders:\r\n","            index = folders.index(fld)\r\n","            for root, dirs, files in os.walk(path + '\\\\' + fld, topdown=False):\r\n","                for file in files:\r\n","                    filepath = root + '\\\\' + file\r\n","                    img = cv2.imread(filepath)\r\n","                    img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\r\n","                    resized = cv2.resize(img, (224, 224), cv2.INTER_LANCZOS4)\r\n","                    y.append(index)\r\n","                    x.append(resized)\r\n","        np.save('{}_X.npy'.format(filename), x)\r\n","        np.save('{}_Y.npy'.format(filename), y)\r\n","\r\n","\r\n","\r\n","    # 預測圖片轉換矩陣\r\n","    @classmethod\r\n","    def load_train_data(self, path=\"D:\\\\NianXiang_File\\\\CNN\\\\Dataset\\\\small_dataset\\\\NEW\", filename=\"\"):\r\n","        x = []\r\n","        y = []\r\n","        folders = []\r\n","        for root, dirs, files in os.walk(path, topdown=False):\r\n","            for name in dirs:\r\n","                folders.append(name)\r\n","        folders = folders[-7:]\r\n","        start_time = time.time()\r\n","        count = 0\r\n","        for fld in folders:\r\n","            index = folders.index(fld)\r\n","            print('Loading {} files (Index: {})'.format(fld, index))\r\n","            for root, dirs, files in os.walk(path + '\\\\' + fld, topdown=False):\r\n","                for file in files:\r\n","                    print(file)\r\n","                    if (file == '1.jpg'):\r\n","                        pass\r\n","                    else:\r\n","                        filepath = root + '\\\\' + file\r\n","                        img = cv2.imread(filepath)\r\n","                        try:\r\n","                            img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\r\n","                            resized = cv2.resize(img, (224, 224), cv2.INTER_LANCZOS4)\r\n","                            y.append(index)\r\n","                            x.append(resized)\r\n","                            count = count + 1\r\n","                        except:\r\n","                            pass\r\n","\r\n","                            # file = Data.static_load_image(ffile)\r\n","        np.save('{}_x.npy'.format(filename), x)\r\n","        np.save('{}_y.npy'.format(filename), y)\r\n","        print(\"Complete\")\r\n","    # 預測圖片轉換矩陣\r\n","    @classmethod\r\n","    def load_predict_data(self, path=\"\", filename=\"\"):\r\n","        x = []\r\n","        for root, dirs, files in os.walk(path, topdown=False):\r\n","            for file in files:\r\n","                filepath = root + '\\\\' + file\r\n","                img = cv2.imread(filepath)\r\n","                img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\r\n","                resized = cv2.resize(img, (224, 224), cv2.INTER_LANCZOS4)\r\n","                x.append(resized)\r\n","        np.save('{}_predict_X.npy'.format(filename), x)\r\n","    # 將圖片抓取臉部後轉換成矩陣\r\n","    @classmethod\r\n","    def get_data_from_path(self, path='D:\\\\NianXiang_File\\\\CNN\\\\Dataset\\\\LE', file_name='BE'):\r\n","        # data = np.load('predict_y_nor.npy')\r\n","        output = {}\r\n","        x = []\r\n","        y = []\r\n","        folders = []\r\n","        for root, dirs, files in os.walk(path, topdown=False):\r\n","            for name in dirs:\r\n","                folders.append(name)\r\n","        folders = folders[-7:]\r\n","        start_time = time.time()\r\n","        count = 0\r\n","        for fld in folders:\r\n","            index = folders.index(fld)\r\n","            print('Loading {} files (Index: {})'.format(fld, index))\r\n","            for root, dirs, files in os.walk(path + '\\\\' + fld, topdown=False):\r\n","                for file in files:\r\n","                    print(file)\r\n","                    if (file == '1.jpg'):\r\n","                        pass\r\n","                    else:\r\n","                        filepath = root + '\\\\' + file\r\n","                        detector = dlib.get_frontal_face_detector()\r\n","                        img = cv2.imread(filepath)\r\n","                        face = detector(img, 1)\r\n","                        for i, d in enumerate(face):\r\n","                            x1 = d.left()\r\n","                            y1 = d.top()\r\n","                            x2 = d.right()\r\n","                            y2 = d.bottom()\r\n","                            img = img[y1:y2, x1:x2]\r\n","                        lpath = 'D:\\\\NianXiang_File\\\\CNN\\\\Dataset\\\\small_dataset\\\\NEW'\r\n","                        filename = lpath + '\\\\' + str(index) + '\\\\' + str(count) + '.jpg'\r\n","                        cv2.imwrite(filename, img)\r\n","                        try:\r\n","                            img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\r\n","                            resized = cv2.resize(img, (224, 224), cv2.INTER_LANCZOS4)\r\n","                            y.append(index)\r\n","                            x.append(resized)\r\n","                            count = count + 1\r\n","                        except:\r\n","                            pass\r\n","\r\n","                            # file = Data.static_load_image(ffile)\r\n","        np.save('{}_X.npy'.format(file_name), x)\r\n","        np.save('{}_Y.npy'.format(file_name), y)\r\n","        print(\"Complete\")\r\n","\r\n","class Evaluate(object):\r\n","    # 評估模型準確率\r\n","    @classmethod\r\n","    def evaluate(self, model, x=None, y=None):\r\n","        loss, score = model.evaluate(x=x, y=y, batch_size=16, verbose=2)\r\n","        print(\"Accuracy：{:.2f}%\".format(score * 100))\r\n","    # 預測資料\r\n","    @classmethod\r\n","    def predict(self, model=None, x=None):\r\n","        predict = model.predict(x=x, batch_size=16, verbose=2)\r\n","        return predict\r\n","    # 混淆矩陣\r\n","    @classmethod\r\n","    def confusion_matrix(self, model=None, normalize=False, title='Confusion Matrix', cls='le', x=None, y=None):\r\n","        predict = model.predict(x=x, batch_size=16, verbose=2)\r\n","        pred = np.argmax(predict, axis=1)\r\n","        print(np.unique(np.argmax(y, axis=1)), np.unique(pred))\r\n","        cnf_matrix = confusion_matrix(np.argmax(y, axis=1), pred)\r\n","        np.set_printoptions(precision=2)\r\n","        plt.figure()\r\n","        if cls == 'be':\r\n","            class_name = ['Angry', 'Disgust', 'Fear', 'Happy', 'Sad', 'Surprise', 'Neutral']\r\n","        if cls == 'le':\r\n","            class_name = [\"Frustration\", \"Confused\", \"Bored\", \"Delightful\", \"Flow\", \"Surprise\"]\r\n","        Evaluate.__plot_confusion_matrix(cnf_matrix, classes=class_name,\r\n","                                title=title,\r\n","                                normalize=normalize)\r\n","        plt.show()\r\n","\r\n","    @classmethod\r\n","    def __plot_confusion_matrix(self, cm, classes,\r\n","                                normalize=False,\r\n","                                title='Confusion matrix',\r\n","                                cmap=plt.cm.Blues):\r\n","        \"\"\"\r\n","        This function prints and plots the confusion matrix.\r\n","        Normalization can be applied by setting `normalize=True`.\r\n","        \"\"\"\r\n","        if normalize:\r\n","            cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\r\n","            print(\"Normalized confusion matrix\")\r\n","        else:\r\n","            print('Confusion matrix, without normalization')\r\n","        print(cm)\r\n","\r\n","        plt.imshow(cm, interpolation='nearest', cmap=cmap)\r\n","        plt.title(title)\r\n","        plt.colorbar()\r\n","        tick_marks = np.arange(len(classes))\r\n","        plt.xticks(tick_marks, classes, rotation=45)\r\n","        plt.yticks(tick_marks, classes)\r\n","\r\n","        fmt = '.2f' if normalize else 'd'\r\n","        thresh = cm.max() / 2.\r\n","        for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\r\n","            plt.text(j, i, format(cm[i, j], fmt),\r\n","                     horizontalalignment=\"center\",\r\n","                     color=\"white\" if cm[i, j] > thresh else \"black\")\r\n","\r\n","        plt.tight_layout()\r\n","        plt.ylabel('True label')\r\n","        plt.xlabel('Predicted label')"],"execution_count":1,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"LpnuSRydkaAG"},"source":["**Train.py 訓練器**"]},{"cell_type":"code","metadata":{"id":"rRXx-QBIkcJj","executionInfo":{"status":"ok","timestamp":1611802043515,"user_tz":-480,"elapsed":4265,"user":{"displayName":"吳貞宜","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhIOzgYhbporG1Eax-zD0HNdC2UZr8CRTfHsZcY5g=s64","userId":"00889427696288622203"}}},"source":["from keras import backend as K\r\n","from keras.layers.normalization import BatchNormalization\r\n","from keras.layers import Flatten, concatenate, Input, Dropout, Dense, Activation, MaxPooling2D, Conv2D, \\\r\n","    AveragePooling2D, GlobalAveragePooling2D, LSTM, TimeDistributed, merge, LeakyReLU\r\n","from keras import optimizers\r\n","from keras.regularizers import l2\r\n","from keras.models import Model, load_model\r\n","from keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\r\n","from keras.utils import np_utils, plot_model, get_custom_objects\r\n","from keras.preprocessing.image import ImageDataGenerator, load_img, img_to_array\r\n","from sklearn.utils import shuffle, class_weight\r\n","from sklearn.model_selection import StratifiedKFold, train_test_split, KFold, cross_val_score\r\n","from sklearn.metrics import classification_report, confusion_matrix\r\n","import gc\r\n","import itertools\r\n","import numpy as np\r\n","import matplotlib.pyplot as plt\r\n","import pandas as pd\r\n","from sklearn.preprocessing import StandardScaler\r\n","# from Package.Data import Data, DataType\r\n","from sklearn.neighbors import KNeighborsClassifier\r\n","\r\n","## 訓練器\r\n","class Trainer(object):\r\n","    def __init__(self, img_width=224, img_height=224, num_classes=7, epochs=50, batch_size=96, data=Data, model=None):\r\n","        self.img_height = img_height\r\n","        self.img_width = img_width\r\n","        self.num_classes = num_classes\r\n","        self.epochs = epochs\r\n","        self.batch_size = batch_size\r\n","        self.set_model(model)\r\n","        self.data = data\r\n","    # 設定模型\r\n","    def set_model(self, model):\r\n","        self._model = model\r\n","\r\n","    # 取得模型\r\n","    def get_model(self):\r\n","        return self._model\r\n","\r\n","    # 訓練模型\r\n","    def fit(self, argument_amount=5, per_epoch_amount=220, filename=\"model\", use_steps=False):\r\n","        if (use_steps):\r\n","            history = self._model.fit_generator(self.data.generator(batch_size=self.batch_size),\r\n","                                                epochs=self.epochs,\r\n","                                                #  steps_per_epoch=int(np.ceil(train_x.shape[0] / float(self.batch_size))),\r\n","                                                workers=4,\r\n","                                                # shuffle=False,\r\n","                                                samples_per_epoch=(len(self.data.train_x) * argument_amount),\r\n","                                                initial_epoch=0,\r\n","                                                validation_data=(self.data.val_x, self.data.val_y),\r\n","                                                # validation_steps=self.batch_size,\r\n","                                                callbacks=self.callbacks,\r\n","                                                verbose=1,\r\n","                                                # class_weight=self.class_weight\r\n","                                                )\r\n","        else:\r\n","            history = self._model.fit_generator(self.data.generator(batch_size=self.batch_size),\r\n","                                                epochs=self.epochs,\r\n","                                                steps_per_epoch=per_epoch_amount,\r\n","                                                #  steps_per_epoch=int(np.ceil(train_x.shape[0] / float(self.batch_size))),\r\n","                                                workers=4,\r\n","                                                # shuffle=False,\r\n","                                                initial_epoch=0,\r\n","                                                validation_data=(self.data.val_x, self.data.val_y),\r\n","                                                # validation_steps=self.batch_size,\r\n","                                                callbacks=self.callbacks,\r\n","                                                verbose=1,\r\n","                                                # class_weight=self.class_weight\r\n","                                                )\r\n","        pd.DataFrame(history.history).to_csv('{}_history.csv'.format(filename))\r\n","    # 建立檢查點與Early Stopping\r\n","    def set_callbacks(self, filename,\r\n","                      check_point=ModelCheckpoint(\"callbacks.{epoch:02d}-{val_acc:.5f}.hdf5\", monitor='val_acc',\r\n","                                         verbose=1,\r\n","                                         save_best_only=True, mode='max'),\r\n","                      earlyStopping=EarlyStopping(monitor='val_acc', min_delta=0.001, patience=10, verbose=2, mode='max'),\r\n","                      reduce_lr=ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=2, min_lr=1E-9)):\r\n","        origin_list = [check_point, earlyStopping, reduce_lr]\r\n","        callbacks = []\r\n","        for item in origin_list:\r\n","            if item is not None:\r\n","                callbacks.append(item)\r\n","        self.callbacks = callbacks"],"execution_count":2,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"_Y-mxx0OnUEu"},"source":["Model.py 模型"]},{"cell_type":"code","metadata":{"id":"InjuAqIZnWVi","executionInfo":{"status":"ok","timestamp":1611802043945,"user_tz":-480,"elapsed":4691,"user":{"displayName":"吳貞宜","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhIOzgYhbporG1Eax-zD0HNdC2UZr8CRTfHsZcY5g=s64","userId":"00889427696288622203"}}},"source":["from keras import backend as K\r\n","from keras.layers.normalization import BatchNormalization\r\n","from keras.layers import Flatten, concatenate, Input, Dropout, Dense, Activation, MaxPooling2D, Conv2D, \\\r\n","    AveragePooling2D, GlobalAveragePooling2D, LSTM, TimeDistributed, merge, LeakyReLU\r\n","from keras import optimizers\r\n","from keras.regularizers import l2\r\n","from keras.models import Model, load_model\r\n","from keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\r\n","from keras.utils import np_utils, plot_model, get_custom_objects\r\n","from keras.preprocessing.image import ImageDataGenerator, load_img, img_to_array\r\n","from sklearn.utils import shuffle, class_weight\r\n","from sklearn.model_selection import StratifiedKFold, train_test_split, KFold\r\n","from sklearn.metrics import classification_report, confusion_matrix\r\n","import gc\r\n","import itertools\r\n","import numpy as np\r\n","import matplotlib.pyplot as plt\r\n","import pandas as pd\r\n","from sklearn.preprocessing import StandardScaler\r\n","\r\n","\r\n","def swish(x):\r\n","    return K.sigmoid(x) * x\r\n","\r\n","\r\n","class Swish(Activation):\r\n","    def __init__(self, activation, **kwargs):\r\n","        super(Swish, self).__init__(activation, **kwargs)\r\n","        self.__name__ = 'swish'\r\n","\r\n","\r\n","get_custom_objects().update({\"swish\": Swish(swish)})\r\n","\r\n","\r\n","# 模型類別\r\n","class CustomModel(object):\r\n","    # 卷積層 + Batch Normalization Layer + Activate Function\r\n","    def conv2d_bn(self, x, filters, num_row, num_col, padding='same', strides=(1, 1), name=None,\r\n","                  activate='relu'):\r\n","        if name is not None:\r\n","            bn_name = name + '_bn'\r\n","            conv_name = name + '_conv'\r\n","        else:\r\n","            bn_name = None\r\n","            conv_name = None\r\n","        x = Conv2D(\r\n","            filters, (num_row, num_col),\r\n","            strides=strides,\r\n","            padding=padding,\r\n","            use_bias=False,\r\n","            kernel_initializer='he_normal',  # globor_uniform,\r\n","            kernel_regularizer = l2(0.0001),\r\n","            name = conv_name)(x)\r\n","        x = BatchNormalization(scale=True, name=bn_name)(x)\r\n","        if (activate != None):\r\n","            x = Activation(activate, name=name)(x)\r\n","        return x\r\n","\r\n","    # 激活模型\r\n","    def model_compile(self, loss='categorical_crossentropy', opt='adamax', met=['acc']):\r\n","        self._model.compile(loss=loss, optimizer=opt, metrics=met)\r\n","\r\n","    # 取得模型\r\n","    def get_model(self):\r\n","        return self._model\r\n","\r\n","    # 設定模型\r\n","    def set_model(self, model):\r\n","        self._model = model\r\n","\r\n","    # 遷移學習\r\n","    def transfer_learning(self, model):\r\n","        model.layers.pop()\r\n","        model.layers.pop()\r\n","        x = model.layers[-1].output\r\n","        x = Dense(7, activation='softmax', name='probbb')(x)#訓練時要改，基本為7，學習為6\r\n","        new_model = Model(inputs=model.input, outputs=x)\r\n","        self._model = new_model\r\n","\r\n","# 論文中提出之架構\r\n","class Dense_FaceLiveNet(CustomModel):\r\n","    def __init__(self, activate='swish', use_dense_block=True, use_global_average_pool=True):\r\n","        # 初始化\r\n","        #   Parameter\r\n","        #   1. activate                 : 激活函數\r\n","        #   2. use_dense_block          : 是否使用 Dense Block，若沒有使用，則為原本 FaceLiveNet 所使用之 Residual Block\r\n","        #   3. use_global_average_pool  : 是否使用 GlobalAveragePool，若沒有使用，則使用全連接層\r\n","        self.activate = activate\r\n","        self.use_dense_block = use_dense_block\r\n","        self.use_global_average_pool = use_global_average_pool\r\n","        self._model = self.build()\r\n","        self.model_compile()\r\n","\r\n","    # 建立模型架構\r\n","    def build(self):\r\n","        get_custom_objects().update({\"swish\": Swish(swish)})\r\n","        # Backend 為 Tensorflow 定義 channel axis 為 3\r\n","        channel_axis = 3\r\n","        # 定義 Input 的大小\r\n","        input_shape = Input(shape=(224, 224, 1), name='data')\r\n","\r\n","        # Stem layer\r\n","        net = self.conv2d_bn(input_shape, 32, 3, 3, strides=(2, 2), padding='valid', activate=self.activate)\r\n","        net = self.conv2d_bn(net, 32, 3, 3, strides=(1, 1), padding='valid', activate=self.activate)\r\n","        net = self.conv2d_bn(net, 64, 3, 3, strides=(1, 1), activate=self.activate)\r\n","        branch_0 = MaxPooling2D((3, 3), strides=(2, 2), padding='valid')(net)\r\n","        net = self.conv2d_bn(branch_0, 80, 3, 3, strides=(2, 2), padding='valid', activate=self.activate)\r\n","        net = self.conv2d_bn(net, 192, 3, 3, strides=(1, 1), padding='valid', activate=self.activate)\r\n","        x = MaxPooling2D((3, 3), strides=(2, 2), padding=\"valid\")(net)\r\n","\r\n","        # inception1\r\n","        branch_0 = self.conv2d_bn(x, 96, 1, 1, strides=(1, 1), activate=self.activate)\r\n","        branch_1 = self.conv2d_bn(x, 64, 1, 1, strides=(1, 1), activate=self.activate)\r\n","        branch_1 = self.conv2d_bn(branch_1, 96, 3, 3, strides=(1, 1), activate=self.activate)\r\n","        branch_2 = self.conv2d_bn(x, 64, 1, 1, strides=(1, 1), activate=self.activate)\r\n","        branch_2 = self.conv2d_bn(branch_2, 96, 3, 3, strides=(1, 1), activate=self.activate)\r\n","        branch_2 = self.conv2d_bn(branch_2, 96, 3, 3, strides=(1, 1), activate=self.activate)\r\n","        x = [branch_0, branch_1, branch_2]\r\n","        mix1 = concatenate(x, axis=channel_axis)\r\n","        x = self.conv2d_bn(mix1, 96, 1, 1, strides=(1, 1), padding='valid', activate=self.activate)\r\n","\r\n","        # inception2\r\n","        branch_0 = self.conv2d_bn(x, 64, 3, 3, strides=(1, 1), activate=self.activate)\r\n","        branch_1 = self.conv2d_bn(x, 96, 1, 1, strides=(1, 1), activate=self.activate)\r\n","        branch_1 = self.conv2d_bn(branch_1, 128, 3, 3, strides=(1, 1), activate=self.activate)\r\n","        branch_1 = self.conv2d_bn(branch_1, 160, 3, 3, strides=(1, 1), activate=self.activate)\r\n","        branch_3 = AveragePooling2D((3, 3), strides=(1, 1), padding='same', name=\"avg_pool_1\")(x)\r\n","        if (self.use_dense_block):\r\n","            x1 = [x, branch_0, branch_1, branch_3]\r\n","        else:\r\n","            x1 = [branch_0, branch_1, branch_3]\r\n","        mix2 = concatenate(x1, axis=channel_axis)\r\n","\r\n","        # inception3\r\n","        branch_0 = self.conv2d_bn(mix2, 192, 1, 1, strides=(1, 1), activate=self.activate)\r\n","        branch_1 = self.conv2d_bn(mix2, 128, 1, 1, strides=(1, 1), activate=self.activate)\r\n","        branch_1 = self.conv2d_bn(branch_1, 160, 1, 7, strides=(1, 1), activate=self.activate)\r\n","        branch_1 = self.conv2d_bn(branch_1, 160, 7, 1, strides=(1, 1), activate=self.activate)\r\n","        if (self.use_dense_block):\r\n","            x = [x, mix2, branch_0, branch_1]\r\n","        else:\r\n","            x = [mix2, branch_0, branch_1]\r\n","        mix3 = concatenate(x, axis=channel_axis, name='mixed3')\r\n","\r\n","        # translate layer\r\n","        if (self.use_dense_block):\r\n","            x = self.conv2d_bn(mix3, 192, 1, 1, strides=(1, 1), padding='valid', activate=self.activate)\r\n","            x = AveragePooling2D((2, 2), strides=(2, 2))(x)\r\n","            x1 = BatchNormalization(scale=True, axis=channel_axis)(x)\r\n","        else:\r\n","            x1 = self.conv2d_bn(mix3, 192, 1, 1, strides=(1, 1), padding='valid', activate=self.activate)\r\n","\r\n","        # inception4\r\n","        netb00 = self.conv2d_bn(x1, 192, 1, 1, strides=(1, 1), padding='same', activate=self.activate)\r\n","        netb10 = self.conv2d_bn(x1, 192, 1, 1, strides=(1, 1), padding='same', activate=self.activate)\r\n","        netb11 = self.conv2d_bn(netb10, 256, 3, 3, strides=(1, 1), padding='same', activate=self.activate)\r\n","        netb20 = self.conv2d_bn(x1, 160, 1, 1, strides=(1, 1), padding='same', activate=self.activate)\r\n","        netb21 = self.conv2d_bn(netb20, 192, 3, 3, strides=(1, 1), padding='same', activate=self.activate)\r\n","        netb22 = self.conv2d_bn(netb21, 256, 3, 3, strides=(1, 1), padding='same', activate=self.activate)\r\n","        netb30 = AveragePooling2D((3, 3), strides=(1, 1), padding='same')(x1)\r\n","        netb31 = self.conv2d_bn(netb30, 160, 1, 1, strides=(1, 1), padding='same', activate=self.activate)\r\n","        if (self.use_dense_block):\r\n","            x = concatenate([x, netb00, netb11, netb22, netb31], axis=channel_axis, name='mixed4')\r\n","        else:\r\n","            x = concatenate([netb00, netb11, netb22, netb31], axis=channel_axis, name='mixed4')\r\n","\r\n","        # inception5 * 2\r\n","        feature_list = [x]\r\n","        for _ in range(2):\r\n","            branch_0 = self.conv2d_bn(x, 256, 1, 1, strides=(1, 1), activate=self.activate)\r\n","            branch_1 = self.conv2d_bn(x, 128, 1, 3, strides=(1, 1), activate=self.activate)\r\n","            branch_1 = self.conv2d_bn(branch_1, 192, 3, 1, strides=(1, 1), activate=self.activate)\r\n","            branch_1 = self.conv2d_bn(branch_1, 256, 1, 3, strides=(1, 1), activate=self.activate)\r\n","            a = [branch_0, branch_1]\r\n","            mix5 = concatenate(a, axis=channel_axis)\r\n","            x1 = self.conv2d_bn(mix5, 256, 1, 1, strides=(1, 1), padding='valid', activate=self.activate)\r\n","            x = concatenate([x, x1], axis=channel_axis)\r\n","            feature_list.append(x)\r\n","        if (self.use_global_average_pool):\r\n","            x = concatenate(feature_list, axis=channel_axis)\r\n","\r\n","        if (self.use_global_average_pool):\r\n","            # GlobalAveragePooling Layer\r\n","            x = GlobalAveragePooling2D(name='global_avg_pool')(x)\r\n","        else:\r\n","            # Fully Connection Layer\r\n","            x = Dense(2000)(x)\r\n","            x = Dense(1000)(x)\r\n","        x = Dense(7, name='Logits')(x)\r\n","        x = Activation('softmax', name='probb')(x)\r\n","        model = Model(inputs=input_shape, outputs=x, name='ANet')\r\n","        return model"],"execution_count":3,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"G94DIE4_yhlE","executionInfo":{"status":"ok","timestamp":1611802047509,"user_tz":-480,"elapsed":8251,"user":{"displayName":"吳貞宜","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhIOzgYhbporG1Eax-zD0HNdC2UZr8CRTfHsZcY5g=s64","userId":"00889427696288622203"}},"outputId":"13b28592-14bf-42af-f06d-f4170dd1ee0b"},"source":["# from Package.Data import Data, DataType, Evaluate\r\n","# from Package.Trainer import *\r\n","import pandas as pd\r\n","import numpy as np\r\n","# from Package.Model import *\r\n","\r\n","if __name__ == '__main__':#改過\r\n","    ## 預測資料\r\n","    model = load_model('/content/drive/MyDrive/basic_emotion/callbacks.01-0.90909.hdf5')#載入訓練好的模型\r\n","    x = Data.static_arr_reshape(arr=np.load('/content/drive/MyDrive/basic_emotion/JAFFE_test_x.npy'))  # 載入X.test\r\n","    predict = Evaluate.predict(model=model, x=x)\r\n","    np.save('JAFFE_predict.npy', predict)\r\n","    print(predict)\r\n","    ## 資料輸出\r\n","    data = np.load('JAFFE_predict.npy')\r\n","    pred = np.argmax(data, axis=1)\r\n","    index = [x for x in range(1, len(pred) + 1)]\r\n","    df = pd.DataFrame(pred, columns=['predict'], index=index)\r\n","    print(df)\r\n","    df.to_csv('JAFFE_predict.csv', sep=',', encoding='utf-8')\r\n"],"execution_count":4,"outputs":[{"output_type":"stream","text":["3/3 - 2s\n","[[9.96340521e-09 2.08339834e-09 2.64180271e-05 7.16434670e-06\n","  6.86290524e-10 9.99961734e-01 4.68906592e-06]\n"," [2.60522711e-06 1.21159394e-06 1.45651693e-05 9.99873638e-01\n","  1.56411152e-05 8.93494434e-05 2.93254834e-06]\n"," [2.47310624e-07 1.62083597e-05 9.99882698e-01 1.08566201e-05\n","  1.40665234e-05 7.10902532e-05 4.85649207e-06]\n"," [9.98068392e-01 2.69683969e-05 1.51394238e-06 4.11198198e-05\n","  2.67231571e-05 1.33147742e-05 1.82186579e-03]\n"," [9.99063790e-01 1.70284227e-04 7.52584901e-06 2.92852910e-05\n","  1.00402853e-04 8.20057176e-06 6.20467938e-04]\n"," [4.88100932e-06 9.98665929e-01 1.23419252e-03 2.05995349e-07\n","  9.43723280e-05 3.37488927e-07 2.89601410e-08]\n"," [1.20044252e-09 9.06941675e-11 5.16251987e-07 9.97748202e-07\n","  5.18702199e-11 9.99997497e-01 9.10691199e-07]\n"," [4.37178300e-04 6.17790590e-07 3.82960352e-05 7.91791754e-05\n","  3.57735485e-06 1.76284942e-04 9.99264896e-01]\n"," [3.11615366e-07 1.29867850e-08 1.65933725e-05 2.29156958e-05\n","  6.39536513e-09 9.99739349e-01 2.20856862e-04]\n"," [9.97414351e-01 2.31342365e-05 1.24430778e-06 5.18030356e-05\n","  7.51014668e-05 1.17703412e-05 2.42248503e-03]\n"," [8.05284117e-07 4.26007261e-07 2.91746401e-05 9.98854756e-01\n","  1.85635345e-05 1.08197564e-03 1.42510562e-05]\n"," [3.09662952e-04 6.03292847e-07 7.61414194e-05 6.27504851e-05\n","  5.89904903e-06 1.88902210e-04 9.99356091e-01]\n"," [1.87406331e-05 9.99837518e-01 6.90842789e-05 1.06650432e-07\n","  7.42824050e-05 1.93872978e-07 3.11908650e-08]\n"," [6.14182136e-06 3.22216101e-06 5.93912910e-06 2.17096340e-06\n","  9.99979854e-01 3.53559955e-08 2.64758978e-06]\n"," [6.97079522e-04 1.28367552e-04 1.96562335e-01 2.02260464e-01\n","  1.39956619e-03 3.09271723e-01 2.89680392e-01]\n"," [4.54029621e-04 6.15567260e-05 1.35533133e-04 1.42823765e-05\n","  9.99230146e-01 1.07997198e-06 1.03405560e-04]\n"," [2.39542453e-09 3.35365402e-10 4.61184709e-06 1.25209874e-06\n","  5.36909683e-11 9.99993563e-01 6.08026028e-07]\n"," [1.31143895e-06 4.32495568e-07 1.35026939e-05 9.99588430e-01\n","  1.86998877e-05 3.59609607e-04 1.79537237e-05]\n"," [2.10519424e-09 3.59758084e-10 5.36133621e-06 2.74305694e-06\n","  1.00759096e-10 9.99991298e-01 5.99141856e-07]\n"," [9.99912024e-01 5.54116668e-05 9.13213626e-07 2.67938594e-06\n","  2.41576089e-07 1.64771154e-06 2.70876408e-05]\n"," [6.02143293e-04 2.28493764e-06 2.03629897e-04 2.50893383e-04\n","  1.20774785e-05 1.20556261e-03 9.97723401e-01]\n"," [1.50619897e-07 4.51982574e-04 9.99453843e-01 2.61350920e-07\n","  9.21034152e-05 1.22886843e-06 4.93006269e-07]\n"," [2.89624097e-07 5.63349386e-06 9.26474968e-06 6.97084062e-08\n","  9.99984503e-01 3.96096134e-09 2.39108232e-07]\n"," [4.31263266e-04 5.10447194e-07 2.94460260e-05 6.18659324e-05\n","  2.72426951e-06 1.12630085e-04 9.99361575e-01]\n"," [1.15323579e-03 7.98353140e-05 1.04112172e-04 9.95757878e-01\n","  7.32261455e-04 3.55313910e-04 1.81730511e-03]\n"," [2.15421494e-08 1.48731363e-06 9.99718726e-01 1.66354687e-06\n","  5.58444935e-06 2.67696800e-04 4.80896642e-06]\n"," [9.99161959e-01 6.83019025e-05 1.65017434e-06 8.86180987e-06\n","  2.80264230e-05 6.00586100e-06 7.25382590e-04]\n"," [3.15982561e-06 4.78333595e-06 5.67168056e-04 4.01482566e-06\n","  9.99343574e-01 4.23644877e-07 7.67904276e-05]\n"," [2.89809657e-04 1.78809489e-06 1.90395644e-04 1.20012213e-04\n","  5.14729727e-05 1.55104412e-04 9.99191463e-01]\n"," [2.66700605e-04 1.84028835e-07 1.36558956e-05 3.30355324e-05\n","  2.28247313e-06 2.41388712e-04 9.99442756e-01]\n"," [2.49448021e-05 4.26159604e-05 2.28751960e-04 9.99239564e-01\n","  1.53093803e-04 2.95050151e-04 1.59412284e-05]\n"," [1.66812952e-05 7.46603109e-06 6.77619319e-05 2.81037660e-06\n","  9.99882817e-01 1.90834953e-07 2.21387854e-05]\n"," [4.38362304e-06 1.23880000e-05 3.38316886e-06 5.69899612e-07\n","  9.99979019e-01 7.72617614e-09 1.86727036e-07]\n"," [1.86147634e-04 8.55756980e-07 2.03212723e-04 1.06601685e-04\n","  1.65580732e-05 2.00070601e-04 9.99286592e-01]\n"," [1.19926088e-04 4.10291108e-07 4.10180801e-05 5.08320845e-05\n","  3.64458974e-05 6.30705326e-05 9.99688268e-01]\n"," [3.78102492e-08 9.99999285e-01 5.45795103e-07 5.64755187e-10\n","  6.60170656e-08 1.68517300e-09 1.31105665e-11]\n"," [4.25175995e-06 9.96993542e-01 2.53932993e-03 4.73963325e-07\n","  4.61261458e-04 9.03164164e-07 1.48427887e-07]\n"," [4.77490714e-04 4.60699312e-07 2.59619337e-05 3.77658507e-05\n","  2.89054401e-06 9.35709104e-05 9.99361813e-01]\n"," [1.40536173e-07 9.99998569e-01 2.25913553e-07 1.21671881e-08\n","  1.08542690e-06 4.74725903e-09 9.42828385e-11]\n"," [3.70228986e-10 1.96683766e-10 1.73761564e-05 3.11708482e-06\n","  9.39604020e-11 9.99979377e-01 1.53008628e-07]\n"," [9.13108593e-08 1.66474763e-08 1.51549408e-04 2.98161594e-05\n","  6.20877083e-09 9.99755561e-01 6.29428177e-05]\n"," [2.67404033e-07 8.05586933e-06 9.99789417e-01 1.28300826e-06\n","  7.00091987e-05 8.16400498e-05 4.93367988e-05]\n"," [9.99795020e-01 8.35371902e-05 2.05825745e-06 8.07575270e-06\n","  1.24221660e-05 3.30958233e-06 9.55578944e-05]]\n","    predict\n","1         5\n","2         3\n","3         2\n","4         0\n","5         0\n","6         1\n","7         5\n","8         6\n","9         5\n","10        0\n","11        3\n","12        6\n","13        1\n","14        4\n","15        5\n","16        4\n","17        5\n","18        3\n","19        5\n","20        0\n","21        6\n","22        2\n","23        4\n","24        6\n","25        3\n","26        2\n","27        0\n","28        4\n","29        6\n","30        6\n","31        3\n","32        4\n","33        4\n","34        6\n","35        6\n","36        1\n","37        1\n","38        6\n","39        1\n","40        5\n","41        5\n","42        2\n","43        0\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"bymZOlPpztc2","executionInfo":{"status":"ok","timestamp":1611802047951,"user_tz":-480,"elapsed":8688,"user":{"displayName":"吳貞宜","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhIOzgYhbporG1Eax-zD0HNdC2UZr8CRTfHsZcY5g=s64","userId":"00889427696288622203"}},"outputId":"e6dd6cef-6e3a-4b62-a927-b534dbfff76a"},"source":["import imageio\r\n","imgs_test = np.load('/content/drive/MyDrive/basic_emotion/JAFFE_test_x.npy')  # 读入.npy文件\r\n","print(imgs_test.shape)\r\n","for i in range(imgs_test.shape[0]):\r\n","  B = imgs_test[i, :, :, 0]\r\n","  imageio.imwrite(\"/content/drive/MyDrive/basic_emotion/predict_result/\" + str(i) + \"_testResults\" + \".jpg\", B)        \r\n","for layer in model.layers:\r\n","  print(layer.output_shape)"],"execution_count":5,"outputs":[{"output_type":"stream","text":["(43, 224, 224, 1)\n","[(None, 224, 224, 1)]\n","(None, 111, 111, 32)\n","(None, 111, 111, 32)\n","(None, 111, 111, 32)\n","(None, 109, 109, 32)\n","(None, 109, 109, 32)\n","(None, 109, 109, 32)\n","(None, 109, 109, 64)\n","(None, 109, 109, 64)\n","(None, 109, 109, 64)\n","(None, 54, 54, 64)\n","(None, 26, 26, 80)\n","(None, 26, 26, 80)\n","(None, 26, 26, 80)\n","(None, 24, 24, 192)\n","(None, 24, 24, 192)\n","(None, 24, 24, 192)\n","(None, 11, 11, 192)\n","(None, 11, 11, 64)\n","(None, 11, 11, 64)\n","(None, 11, 11, 64)\n","(None, 11, 11, 64)\n","(None, 11, 11, 96)\n","(None, 11, 11, 64)\n","(None, 11, 11, 96)\n","(None, 11, 11, 64)\n","(None, 11, 11, 96)\n","(None, 11, 11, 96)\n","(None, 11, 11, 96)\n","(None, 11, 11, 96)\n","(None, 11, 11, 96)\n","(None, 11, 11, 96)\n","(None, 11, 11, 96)\n","(None, 11, 11, 96)\n","(None, 11, 11, 96)\n","(None, 11, 11, 96)\n","(None, 11, 11, 288)\n","(None, 11, 11, 96)\n","(None, 11, 11, 96)\n","(None, 11, 11, 96)\n","(None, 11, 11, 96)\n","(None, 11, 11, 96)\n","(None, 11, 11, 96)\n","(None, 11, 11, 128)\n","(None, 11, 11, 128)\n","(None, 11, 11, 128)\n","(None, 11, 11, 64)\n","(None, 11, 11, 160)\n","(None, 11, 11, 64)\n","(None, 11, 11, 160)\n","(None, 11, 11, 64)\n","(None, 11, 11, 160)\n","(None, 11, 11, 96)\n","(None, 11, 11, 416)\n","(None, 11, 11, 128)\n","(None, 11, 11, 128)\n","(None, 11, 11, 128)\n","(None, 11, 11, 160)\n","(None, 11, 11, 160)\n","(None, 11, 11, 160)\n","(None, 11, 11, 192)\n","(None, 11, 11, 160)\n","(None, 11, 11, 192)\n","(None, 11, 11, 160)\n","(None, 11, 11, 192)\n","(None, 11, 11, 160)\n","(None, 11, 11, 864)\n","(None, 11, 11, 192)\n","(None, 11, 11, 192)\n","(None, 11, 11, 192)\n","(None, 5, 5, 192)\n","(None, 5, 5, 192)\n","(None, 5, 5, 160)\n","(None, 5, 5, 160)\n","(None, 5, 5, 160)\n","(None, 5, 5, 192)\n","(None, 5, 5, 192)\n","(None, 5, 5, 192)\n","(None, 5, 5, 192)\n","(None, 5, 5, 192)\n","(None, 5, 5, 192)\n","(None, 5, 5, 192)\n","(None, 5, 5, 192)\n","(None, 5, 5, 256)\n","(None, 5, 5, 256)\n","(None, 5, 5, 160)\n","(None, 5, 5, 192)\n","(None, 5, 5, 256)\n","(None, 5, 5, 256)\n","(None, 5, 5, 160)\n","(None, 5, 5, 192)\n","(None, 5, 5, 256)\n","(None, 5, 5, 256)\n","(None, 5, 5, 160)\n","(None, 5, 5, 1056)\n","(None, 5, 5, 128)\n","(None, 5, 5, 128)\n","(None, 5, 5, 128)\n","(None, 5, 5, 192)\n","(None, 5, 5, 192)\n","(None, 5, 5, 192)\n","(None, 5, 5, 256)\n","(None, 5, 5, 256)\n","(None, 5, 5, 256)\n","(None, 5, 5, 256)\n","(None, 5, 5, 256)\n","(None, 5, 5, 256)\n","(None, 5, 5, 512)\n","(None, 5, 5, 256)\n","(None, 5, 5, 256)\n","(None, 5, 5, 256)\n","(None, 5, 5, 1312)\n","(None, 5, 5, 128)\n","(None, 5, 5, 128)\n","(None, 5, 5, 128)\n","(None, 5, 5, 192)\n","(None, 5, 5, 192)\n","(None, 5, 5, 192)\n","(None, 5, 5, 256)\n","(None, 5, 5, 256)\n","(None, 5, 5, 256)\n","(None, 5, 5, 256)\n","(None, 5, 5, 256)\n","(None, 5, 5, 256)\n","(None, 5, 5, 512)\n","(None, 5, 5, 256)\n","(None, 5, 5, 256)\n","(None, 5, 5, 256)\n","(None, 5, 5, 1568)\n","(None, 5, 5, 3936)\n","(None, 3936)\n","(None, 7)\n"],"name":"stdout"}]}]}